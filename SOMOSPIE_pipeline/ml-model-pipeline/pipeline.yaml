apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: somospiepipeline
  annotations:
    tekton.dev/output_artifacts: '{"knn-inference": [{"key": "artifacts/$PIPELINERUN/knn-inference/Output.tgz",
      "name": "knn-inference-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-10":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-10/Output.tgz", "name": "knn-inference-10-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-11": [{"key": "artifacts/$PIPELINERUN/knn-inference-11/Output.tgz",
      "name": "knn-inference-11-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-12":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-12/Output.tgz", "name": "knn-inference-12-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-13": [{"key": "artifacts/$PIPELINERUN/knn-inference-13/Output.tgz",
      "name": "knn-inference-13-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-14":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-14/Output.tgz", "name": "knn-inference-14-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-15": [{"key": "artifacts/$PIPELINERUN/knn-inference-15/Output.tgz",
      "name": "knn-inference-15-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-16":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-16/Output.tgz", "name": "knn-inference-16-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-17": [{"key": "artifacts/$PIPELINERUN/knn-inference-17/Output.tgz",
      "name": "knn-inference-17-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-18":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-18/Output.tgz", "name": "knn-inference-18-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-19": [{"key": "artifacts/$PIPELINERUN/knn-inference-19/Output.tgz",
      "name": "knn-inference-19-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-2":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-2/Output.tgz", "name": "knn-inference-2-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-20": [{"key": "artifacts/$PIPELINERUN/knn-inference-20/Output.tgz",
      "name": "knn-inference-20-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-21":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-21/Output.tgz", "name": "knn-inference-21-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-22": [{"key": "artifacts/$PIPELINERUN/knn-inference-22/Output.tgz",
      "name": "knn-inference-22-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-23":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-23/Output.tgz", "name": "knn-inference-23-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-24": [{"key": "artifacts/$PIPELINERUN/knn-inference-24/Output.tgz",
      "name": "knn-inference-24-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-25":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-25/Output.tgz", "name": "knn-inference-25-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-26": [{"key": "artifacts/$PIPELINERUN/knn-inference-26/Output.tgz",
      "name": "knn-inference-26-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-27":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-27/Output.tgz", "name": "knn-inference-27-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-3": [{"key": "artifacts/$PIPELINERUN/knn-inference-3/Output.tgz",
      "name": "knn-inference-3-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-4":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-4/Output.tgz", "name": "knn-inference-4-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-5": [{"key": "artifacts/$PIPELINERUN/knn-inference-5/Output.tgz",
      "name": "knn-inference-5-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-6":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-6/Output.tgz", "name": "knn-inference-6-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-7": [{"key": "artifacts/$PIPELINERUN/knn-inference-7/Output.tgz",
      "name": "knn-inference-7-Output", "path": "/tmp/outputs/Output/data"}], "knn-inference-8":
      [{"key": "artifacts/$PIPELINERUN/knn-inference-8/Output.tgz", "name": "knn-inference-8-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-inference-9": [{"key": "artifacts/$PIPELINERUN/knn-inference-9/Output.tgz",
      "name": "knn-inference-9-Output", "path": "/tmp/outputs/Output/data"}], "knn-train":
      [{"key": "artifacts/$PIPELINERUN/knn-train/Output.tgz", "name": "knn-train-Output",
      "path": "/tmp/outputs/Output/data"}], "knn-train-2": [{"key": "artifacts/$PIPELINERUN/knn-train-2/Output.tgz",
      "name": "knn-train-2-Output", "path": "/tmp/outputs/Output/data"}], "knn-train-3":
      [{"key": "artifacts/$PIPELINERUN/knn-train-3/Output.tgz", "name": "knn-train-3-Output",
      "path": "/tmp/outputs/Output/data"}], "load-data": [{"key": "artifacts/$PIPELINERUN/load-data/data.tgz",
      "name": "load-data-data", "path": "/tmp/outputs/data/data"}, {"key": "artifacts/$PIPELINERUN/load-data/scaler.tgz",
      "name": "load-data-scaler", "path": "/tmp/outputs/scaler/data"}], "load-data-2":
      [{"key": "artifacts/$PIPELINERUN/load-data-2/data.tgz", "name": "load-data-2-data",
      "path": "/tmp/outputs/data/data"}, {"key": "artifacts/$PIPELINERUN/load-data-2/scaler.tgz",
      "name": "load-data-2-scaler", "path": "/tmp/outputs/scaler/data"}], "load-data-3":
      [{"key": "artifacts/$PIPELINERUN/load-data-3/data.tgz", "name": "load-data-3-data",
      "path": "/tmp/outputs/data/data"}, {"key": "artifacts/$PIPELINERUN/load-data-3/scaler.tgz",
      "name": "load-data-3-scaler", "path": "/tmp/outputs/scaler/data"}], "rf-inference":
      [{"key": "artifacts/$PIPELINERUN/rf-inference/Output.tgz", "name": "rf-inference-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-10": [{"key": "artifacts/$PIPELINERUN/rf-inference-10/Output.tgz",
      "name": "rf-inference-10-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-11":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-11/Output.tgz", "name": "rf-inference-11-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-12": [{"key": "artifacts/$PIPELINERUN/rf-inference-12/Output.tgz",
      "name": "rf-inference-12-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-13":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-13/Output.tgz", "name": "rf-inference-13-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-14": [{"key": "artifacts/$PIPELINERUN/rf-inference-14/Output.tgz",
      "name": "rf-inference-14-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-15":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-15/Output.tgz", "name": "rf-inference-15-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-16": [{"key": "artifacts/$PIPELINERUN/rf-inference-16/Output.tgz",
      "name": "rf-inference-16-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-17":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-17/Output.tgz", "name": "rf-inference-17-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-18": [{"key": "artifacts/$PIPELINERUN/rf-inference-18/Output.tgz",
      "name": "rf-inference-18-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-19":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-19/Output.tgz", "name": "rf-inference-19-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-2": [{"key": "artifacts/$PIPELINERUN/rf-inference-2/Output.tgz",
      "name": "rf-inference-2-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-20":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-20/Output.tgz", "name": "rf-inference-20-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-21": [{"key": "artifacts/$PIPELINERUN/rf-inference-21/Output.tgz",
      "name": "rf-inference-21-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-22":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-22/Output.tgz", "name": "rf-inference-22-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-23": [{"key": "artifacts/$PIPELINERUN/rf-inference-23/Output.tgz",
      "name": "rf-inference-23-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-24":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-24/Output.tgz", "name": "rf-inference-24-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-25": [{"key": "artifacts/$PIPELINERUN/rf-inference-25/Output.tgz",
      "name": "rf-inference-25-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-26":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-26/Output.tgz", "name": "rf-inference-26-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-27": [{"key": "artifacts/$PIPELINERUN/rf-inference-27/Output.tgz",
      "name": "rf-inference-27-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-3":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-3/Output.tgz", "name": "rf-inference-3-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-4": [{"key": "artifacts/$PIPELINERUN/rf-inference-4/Output.tgz",
      "name": "rf-inference-4-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-5":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-5/Output.tgz", "name": "rf-inference-5-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-6": [{"key": "artifacts/$PIPELINERUN/rf-inference-6/Output.tgz",
      "name": "rf-inference-6-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-7":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-7/Output.tgz", "name": "rf-inference-7-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-inference-8": [{"key": "artifacts/$PIPELINERUN/rf-inference-8/Output.tgz",
      "name": "rf-inference-8-Output", "path": "/tmp/outputs/Output/data"}], "rf-inference-9":
      [{"key": "artifacts/$PIPELINERUN/rf-inference-9/Output.tgz", "name": "rf-inference-9-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-train": [{"key": "artifacts/$PIPELINERUN/rf-train/Output.tgz",
      "name": "rf-train-Output", "path": "/tmp/outputs/Output/data"}], "rf-train-2":
      [{"key": "artifacts/$PIPELINERUN/rf-train-2/Output.tgz", "name": "rf-train-2-Output",
      "path": "/tmp/outputs/Output/data"}], "rf-train-3": [{"key": "artifacts/$PIPELINERUN/rf-train-3/Output.tgz",
      "name": "rf-train-3-Output", "path": "/tmp/outputs/Output/data"}]}'
    tekton.dev/input_artifacts: '{"knn-inference": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-10": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-11": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-12": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-13": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-14": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-15": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-16": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-17": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-18": [{"name": "knn-train-2-Output", "parent_task":
      "knn-train-2"}, {"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-19": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-2": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-20": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-21": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-22": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-23": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-24": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-25": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-26": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-27": [{"name": "knn-train-3-Output", "parent_task":
      "knn-train-3"}, {"name": "load-data-3-scaler", "parent_task": "load-data-3"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-3": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-4": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-5": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-6": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-7": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-8": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-inference-9": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "load-data-scaler", "parent_task": "load-data"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}], "knn-train": [{"name": "load-data-data", "parent_task": "load-data"},
      {"name": "pvc-train-name", "parent_task": "pvc-train"}], "knn-train-2": [{"name":
      "load-data-2-data", "parent_task": "load-data-2"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "knn-train-3": [{"name": "load-data-3-data", "parent_task":
      "load-data-3"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}], "load-data":
      [{"name": "pvc-train-name", "parent_task": "pvc-train"}], "load-data-2": [{"name":
      "pvc-train-name", "parent_task": "pvc-train"}], "load-data-3": [{"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "rf-inference": [{"name": "load-data-scaler",
      "parent_task": "load-data"}, {"name": "pvc-eval-name", "parent_task": "pvc-eval"},
      {"name": "pvc-predictions-name", "parent_task": "pvc-predictions"}, {"name":
      "pvc-train-name", "parent_task": "pvc-train"}, {"name": "rf-train-Output", "parent_task":
      "rf-train"}], "rf-inference-10": [{"name": "load-data-2-scaler", "parent_task":
      "load-data-2"}, {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name":
      "pvc-predictions-name", "parent_task": "pvc-predictions"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}, {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}],
      "rf-inference-11": [{"name": "load-data-2-scaler", "parent_task": "load-data-2"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}, {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}], "rf-inference-12":
      [{"name": "load-data-2-scaler", "parent_task": "load-data-2"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}], "rf-inference-13":
      [{"name": "load-data-2-scaler", "parent_task": "load-data-2"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}], "rf-inference-14":
      [{"name": "load-data-2-scaler", "parent_task": "load-data-2"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}], "rf-inference-15":
      [{"name": "load-data-2-scaler", "parent_task": "load-data-2"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}], "rf-inference-16":
      [{"name": "load-data-2-scaler", "parent_task": "load-data-2"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}], "rf-inference-17":
      [{"name": "load-data-2-scaler", "parent_task": "load-data-2"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}], "rf-inference-18":
      [{"name": "load-data-2-scaler", "parent_task": "load-data-2"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-2-Output", "parent_task": "rf-train-2"}], "rf-inference-19":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-2":
      [{"name": "load-data-scaler", "parent_task": "load-data"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-Output", "parent_task": "rf-train"}], "rf-inference-20":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-21":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-22":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-23":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-24":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-25":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-26":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-27":
      [{"name": "load-data-3-scaler", "parent_task": "load-data-3"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-3-Output", "parent_task": "rf-train-3"}], "rf-inference-3":
      [{"name": "load-data-scaler", "parent_task": "load-data"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-Output", "parent_task": "rf-train"}], "rf-inference-4": [{"name":
      "load-data-scaler", "parent_task": "load-data"}, {"name": "pvc-eval-name", "parent_task":
      "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task": "pvc-predictions"},
      {"name": "pvc-train-name", "parent_task": "pvc-train"}, {"name": "rf-train-Output",
      "parent_task": "rf-train"}], "rf-inference-5": [{"name": "load-data-scaler",
      "parent_task": "load-data"}, {"name": "pvc-eval-name", "parent_task": "pvc-eval"},
      {"name": "pvc-predictions-name", "parent_task": "pvc-predictions"}, {"name":
      "pvc-train-name", "parent_task": "pvc-train"}, {"name": "rf-train-Output", "parent_task":
      "rf-train"}], "rf-inference-6": [{"name": "load-data-scaler", "parent_task":
      "load-data"}, {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name":
      "pvc-predictions-name", "parent_task": "pvc-predictions"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}, {"name": "rf-train-Output", "parent_task": "rf-train"}],
      "rf-inference-7": [{"name": "load-data-scaler", "parent_task": "load-data"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name",
      "parent_task": "pvc-predictions"}, {"name": "pvc-train-name", "parent_task":
      "pvc-train"}, {"name": "rf-train-Output", "parent_task": "rf-train"}], "rf-inference-8":
      [{"name": "load-data-scaler", "parent_task": "load-data"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task":
      "pvc-predictions"}, {"name": "pvc-train-name", "parent_task": "pvc-train"},
      {"name": "rf-train-Output", "parent_task": "rf-train"}], "rf-inference-9": [{"name":
      "load-data-scaler", "parent_task": "load-data"}, {"name": "pvc-eval-name", "parent_task":
      "pvc-eval"}, {"name": "pvc-predictions-name", "parent_task": "pvc-predictions"},
      {"name": "pvc-train-name", "parent_task": "pvc-train"}, {"name": "rf-train-Output",
      "parent_task": "rf-train"}], "rf-train": [{"name": "load-data-data", "parent_task":
      "load-data"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}], "rf-train-2":
      [{"name": "load-data-2-data", "parent_task": "load-data-2"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "rf-train-3": [{"name": "load-data-3-data", "parent_task":
      "load-data-3"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"knn-inference": [["Output", "$(results.Output.path)"]],
      "knn-inference-10": [["Output", "$(results.Output.path)"]], "knn-inference-11":
      [["Output", "$(results.Output.path)"]], "knn-inference-12": [["Output", "$(results.Output.path)"]],
      "knn-inference-13": [["Output", "$(results.Output.path)"]], "knn-inference-14":
      [["Output", "$(results.Output.path)"]], "knn-inference-15": [["Output", "$(results.Output.path)"]],
      "knn-inference-16": [["Output", "$(results.Output.path)"]], "knn-inference-17":
      [["Output", "$(results.Output.path)"]], "knn-inference-18": [["Output", "$(results.Output.path)"]],
      "knn-inference-19": [["Output", "$(results.Output.path)"]], "knn-inference-2":
      [["Output", "$(results.Output.path)"]], "knn-inference-20": [["Output", "$(results.Output.path)"]],
      "knn-inference-21": [["Output", "$(results.Output.path)"]], "knn-inference-22":
      [["Output", "$(results.Output.path)"]], "knn-inference-23": [["Output", "$(results.Output.path)"]],
      "knn-inference-24": [["Output", "$(results.Output.path)"]], "knn-inference-25":
      [["Output", "$(results.Output.path)"]], "knn-inference-26": [["Output", "$(results.Output.path)"]],
      "knn-inference-27": [["Output", "$(results.Output.path)"]], "knn-inference-3":
      [["Output", "$(results.Output.path)"]], "knn-inference-4": [["Output", "$(results.Output.path)"]],
      "knn-inference-5": [["Output", "$(results.Output.path)"]], "knn-inference-6":
      [["Output", "$(results.Output.path)"]], "knn-inference-7": [["Output", "$(results.Output.path)"]],
      "knn-inference-8": [["Output", "$(results.Output.path)"]], "knn-inference-9":
      [["Output", "$(results.Output.path)"]], "knn-train": [["Output", "$(results.Output.path)"]],
      "knn-train-2": [["Output", "$(results.Output.path)"]], "knn-train-3": [["Output",
      "$(results.Output.path)"]], "load-data": [["data", "$(results.data.path)"],
      ["scaler", "$(results.scaler.path)"]], "load-data-2": [["data", "$(results.data.path)"],
      ["scaler", "$(results.scaler.path)"]], "load-data-3": [["data", "$(results.data.path)"],
      ["scaler", "$(results.scaler.path)"]], "pvc-eval": [], "pvc-predictions": [],
      "pvc-train": [], "rf-inference": [["Output", "$(results.Output.path)"]], "rf-inference-10":
      [["Output", "$(results.Output.path)"]], "rf-inference-11": [["Output", "$(results.Output.path)"]],
      "rf-inference-12": [["Output", "$(results.Output.path)"]], "rf-inference-13":
      [["Output", "$(results.Output.path)"]], "rf-inference-14": [["Output", "$(results.Output.path)"]],
      "rf-inference-15": [["Output", "$(results.Output.path)"]], "rf-inference-16":
      [["Output", "$(results.Output.path)"]], "rf-inference-17": [["Output", "$(results.Output.path)"]],
      "rf-inference-18": [["Output", "$(results.Output.path)"]], "rf-inference-19":
      [["Output", "$(results.Output.path)"]], "rf-inference-2": [["Output", "$(results.Output.path)"]],
      "rf-inference-20": [["Output", "$(results.Output.path)"]], "rf-inference-21":
      [["Output", "$(results.Output.path)"]], "rf-inference-22": [["Output", "$(results.Output.path)"]],
      "rf-inference-23": [["Output", "$(results.Output.path)"]], "rf-inference-24":
      [["Output", "$(results.Output.path)"]], "rf-inference-25": [["Output", "$(results.Output.path)"]],
      "rf-inference-26": [["Output", "$(results.Output.path)"]], "rf-inference-27":
      [["Output", "$(results.Output.path)"]], "rf-inference-3": [["Output", "$(results.Output.path)"]],
      "rf-inference-4": [["Output", "$(results.Output.path)"]], "rf-inference-5":
      [["Output", "$(results.Output.path)"]], "rf-inference-6": [["Output", "$(results.Output.path)"]],
      "rf-inference-7": [["Output", "$(results.Output.path)"]], "rf-inference-8":
      [["Output", "$(results.Output.path)"]], "rf-inference-9": [["Output", "$(results.Output.path)"]],
      "rf-train": [["Output", "$(results.Output.path)"]], "rf-train-2": [["Output",
      "$(results.Output.path)"]], "rf-train-3": [["Output", "$(results.Output.path)"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Pipeline for somospie",
      "inputs": [{"default": "icr.io/somospie/somospie-gdal-netcdf", "name": "container_image",
      "optional": true, "type": "String"}, {"default": "3", "name": "total_tiles",
      "optional": true, "type": "Integer"}, {"default": "oklahoma-30m-eval", "name":
      "eval_cos", "optional": true, "type": "String"}, {"default": "oklahoma-27km",
      "name": "train_cos", "optional": true, "type": "String"}, {"default": "p-oklahoma-30m",
      "name": "predict_cos", "optional": true, "type": "String"}, {"default": "13",
      "name": "mem_lim", "optional": true, "type": "Integer"}, {"default": "3", "name":
      "random_seed", "optional": true, "type": "Integer"}, {"default": "20", "name":
      "k_neighbors", "optional": true, "type": "Integer"}, {"default": "2000", "name":
      "trees", "optional": true, "type": "Integer"}], "name": "somospiepipeline"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: container_image
    value: icr.io/somospie/somospie-gdal-netcdf
  - name: eval_cos
    value: oklahoma-30m-eval
  - name: k_neighbors
    value: '20'
  - name: mem_lim
    value: '13'
  - name: predict_cos
    value: p-oklahoma-30m
  - name: random_seed
    value: '3'
  - name: total_tiles
    value: '3'
  - name: train_cos
    value: oklahoma-27km
  - name: trees
    value: '2000'
  pipelineSpec:
    params:
    - name: container_image
      default: icr.io/somospie/somospie-gdal-netcdf
    - name: eval_cos
      default: oklahoma-30m-eval
    - name: k_neighbors
      default: '20'
    - name: mem_lim
      default: '13'
    - name: predict_cos
      default: p-oklahoma-30m
    - name: random_seed
      default: '3'
    - name: total_tiles
      default: '3'
    - name: train_cos
      default: oklahoma-27km
    - name: trees
      default: '2000'
    tasks:
    - name: pvc-eval
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: eval_cos
        value: $(params.eval_cos)
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        - name: eval_cos
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: $(inputs.params.eval_cos)
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-odh-eval
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
      retries: 3
    - name: pvc-train
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: train_cos
        value: $(params.train_cos)
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        - name: train_cos
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: $(inputs.params.train_cos)
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-odh-train
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
      retries: 3
    - name: pvc-predictions
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: predict_cos
        value: $(params.predict_cos)
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        - name: predict_cos
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: $(inputs.params.predict_cos)
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-odh-predictions
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
      retries: 3
    - name: load-data
      params:
      - name: container_image
        value: $(params.container_image)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-path
          - /train/2010_02.tif
          - --dir
          - /train/
          - --out-data
          - /train/2010_02.json
          - '----output-paths'
          - $(results.data.path)
          - $(results.scaler.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def load_data(input_path,dir, out_data):
                import numpy as np
                import json
                import pandas as pd
                import pickle
                from sklearn.preprocessing import StandardScaler
                from sklearn.model_selection import train_test_split
                from osgeo import gdal

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                            b = ds.GetRasterBand(band + 1)
                            names.append(b.GetDescription())
                    ds = None
                    return names

                def tif2df(raster_file):
                    ds = gdal.Open(raster_file, 0)
                    xmin, res, _, ymax, _, _ = ds.GetGeoTransform()
                    xsize = ds.RasterXSize
                    ysize = ds.RasterYSize
                    xstart = xmin + res / 2
                    ystart = ymax - res / 2

                    x = np.arange(xstart, xstart + xsize * res, res)
                    y = np.arange(ystart, ystart - ysize * res, -res)
                    x = np.tile(x[:xsize], ysize)
                    y = np.repeat(y[:ysize], xsize)

                    band_names = get_band_names(raster_file)

                    n_bands = ds.RasterCount
                    bands = np.zeros((x.shape[0], n_bands))
                    for k in range(1, n_bands + 1):
                            band = ds.GetRasterBand(k)
                            data = band.ReadAsArray()
                            data = np.ma.array(data, mask=np.equal(data, band.GetNoDataValue()))
                            data = data.filled(np.nan)
                            bands[:, k-1] = data.flatten()

                    column_names = ['x', 'y'] + band_names
                    stack = np.column_stack((x, y, bands))
                    df = pd.DataFrame(stack, columns=column_names)
                    df.dropna(inplace=True)
                    # df.to_csv(output_file, index=None)
                    return df

                print("Reading training data from", input_path)
                training_data = tif2df(input_path)
                print(training_data)

                x_train, x_test, y_train, y_test = train_test_split(training_data.loc[:,training_data.columns != 'z'], training_data.loc[:,'z'], test_size=0.1)
                #print(x_train,"\n",y_train,"TEST\n",x_test,y_test)
                ss = StandardScaler()
                x_train = ss.fit_transform(x_train)
                x_test = ss.transform(x_test)
                #print("SCALED")
                #print(x_train,"\n",y_train,"TEST\n",x_test,y_test)

                # Save scaler model so it can be reused for predicting
                pickle.dump(ss, open(dir+'scaler.pkl', 'wb'))

                # Save data to train with different ml-models
                data = {'x_train' : x_train.tolist(),
                        'y_train' : y_train.tolist(),
                        'x_test' : x_test.tolist(),
                        'y_test' : y_test.tolist()}
                # Creates a json object based on `data`
                data_json = json.dumps(data)

                # Saves the json object into a file
                with open(out_data, 'w') as out_file:
                    json.dump(data_json, out_file)

                   # Create path to where data and scaler are saved

                return [out_data,dir+'scaler.pkl']

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Load data', description='')
            _parser.add_argument("--input-path", dest="input_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--out-data", dest="out_data", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = load_data(**_parsed_args)

            _output_serializers = [
                _serialize_str,
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: pvc-train-name
        results:
        - name: data
          type: string
          description: /tmp/outputs/data/data
        - name: scaler
          type: string
          description: /tmp/outputs/scaler/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Load data", "outputs":
              [{"name": "data", "type": "String"}, {"name": "scaler", "type": "String"}],
              "version": "Load data@sha256=b95e43bb0c0e533f62c81cde1dac345ef7c794440c1eb046738dc9e11e4ff3b5"}'
      retries: 3
    - name: knn-train
      params:
      - name: container_image
        value: $(params.container_image)
      - name: k_neighbors
        value: $(params.k_neighbors)
      - name: load-data-data
        value: $(tasks.load-data.results.data)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: random_seed
        value: $(params.random_seed)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - $(inputs.params.load-data-data)
          - --k
          - $(inputs.params.k_neighbors)
          - --seed
          - $(inputs.params.random_seed)
          - --out-model
          - /train/model_knn1.pkl
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_train (data_path, k, seed, out_model):\n    import numpy as np\n\
            \    import pickle\n    import json\n    from sklearn.neighbors import\
            \ KNeighborsRegressor\n    from sklearn.model_selection import RandomizedSearchCV\n\
            \    from sklearn.metrics import mean_squared_error\n    import os\n\n\
            \    # Define functions for training\n    def random_parameter_search(knn,\
            \ x_train, y_train, maxK, seed):\n        # Dictionary with all the hyperparameter\
            \ options for the knn model: n_neighbors, weights, metric\n        params\
            \ = {'n_neighbors': list(range(2,maxK)),\n            'weights': ['uniform','distance',\
            \ gaussian],\n            'metric': ['euclidean','minkowski']\n      \
            \          }\n        # Random search based on the grid of params and\
            \ n_iter controls number of random combinations it will try\n        #\
            \ n_jobs=-1 means using all processors\n        # random_state sets the\
            \ seed for manner of reproducibility \n        params_search = RandomizedSearchCV(knn,\
            \ params, verbose=1, cv=10, n_iter=50, random_state=seed, n_jobs=-1)\n\
            \        params_search.fit(x_train,y_train)\n        # Check the results\
            \ from the parameter search  \n        print(params_search.best_score_)\n\
            \        print(params_search.best_params_)\n        print(params_search.best_estimator_)\n\
            \        return params_search.best_params_\n\n    def gaussian(dist, sigma\
            \ = 4):\n        # Input a distance and return its weight using the gaussian\
            \ kernel \n        weight = np.exp(-dist**2/(2*sigma**2))\n        return\
            \ weight\n\n    def validate_knn(knn, x_test, y_test):\n        # Predict\
            \ on x_test\n        y_test_predicted = knn.predict(x_test)\n        #\
            \ Measure the rmse\n        rmse = np.sqrt(mean_squared_error(y_test,\
            \ y_test_predicted))\n        # Print error\t\n        #print(\"Predictions\
            \ of soil moisture:\", y_test_predicted)\n        #print(\"Original values\
            \ of soil moisture:\", y_test)\n        print(\"The rmse for the validation\
            \ is:\", rmse)\n\n    print(\"Checking if model \", out_model, \" exists\"\
            )\n    if os.path.isfile(out_model):\n        print(\"The model \", out_model,\
            \ \" exists\")\n        return out_model\n    else:\n        print(\"\
            Reading training data from\", data_path)\n            # Open and reads\
            \ file \"data\"\n        with open(data_path) as data_file:\n        \
            \    data = json.load(data_file)\n\n        data = json.loads(data)\n\n\
            \        x_train = data['x_train']\n        y_train = data['y_train']\n\
            \        x_test = data['x_test']\n        y_test = data['y_test']\n  \
            \      #print(training_data)\n        maxK = int(k)\n        seed = int(seed)\n\
            \n            # Define initial model\n        knn = KNeighborsRegressor(leaf_size=3000)\n\
            \        # Random parameter search of n_neighbors, weigths and metric\n\
            \        best_params = random_parameter_search(knn, x_train, y_train,\
            \ maxK, seed)\n        # Based on selection build the new regressor\n\
            \        knn = KNeighborsRegressor(n_neighbors=best_params['n_neighbors'],\
            \ weights=best_params['weights'],\n                        metric=best_params['metric'],\
            \ n_jobs=-1)\n        # Fit the new model to data\n        knn.fit(x_train,\
            \ y_train)\n        # Save model\n        pickle.dump(knn, open(out_model,\
            \ 'wb'))\n\n        # Validate\n        validate_knn(knn, x_test, y_test)\n\
            \        return out_model\n\ndef _serialize_str(str_value: str) -> str:\n\
            \    if not isinstance(str_value, str):\n        raise TypeError('Value\
            \ \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value),\
            \ str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser\
            \ = argparse.ArgumentParser(prog='Knn train', description='')\n_parser.add_argument(\"\
            --data-path\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--k\", dest=\"k\", type=int, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--seed\", dest=\"seed\", type=int, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-model\", dest=\"\
            out_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = knn_train(**_parsed_args)\n\n_outputs = [_outputs]\n\
            \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor\
            \ idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: k_neighbors
        - name: load-data-data
        - name: pvc-train-name
        - name: random_seed
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn train", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Knn train@sha256=2d6f1c535600b8f4370aa2afe20e9b3e914423a0b861ea009a5434a4de95316d"}'
      retries: 3
    - name: rf-train
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-data
        value: $(tasks.load-data.results.data)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: random_seed
        value: $(params.random_seed)
      - name: trees
        value: $(params.trees)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - $(inputs.params.load-data-data)
          - --maxtree
          - $(inputs.params.trees)
          - --seed
          - $(inputs.params.random_seed)
          - --out-model
          - /train/model_rf1.pkl
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_train(data_path, maxtree, seed, out_model):\n    # Libraries\n\
            \    import numpy as np\n    import json\n    import pickle\n    from\
            \ sklearn.ensemble import RandomForestRegressor\n    from sklearn.model_selection\
            \ import RandomizedSearchCV\n    from sklearn.metrics import mean_squared_error\n\
            \    import os\n\n    # Functions \n    def random_parameter_search(rf,\
            \ x_train, y_train, maxtree, seed):\n        # Number of trees in random\
            \ forest\n        n_estimators = [int(x) for x in np.linspace(start =\
            \ 300, stop = maxtree, num = 100)]\n        # Number of features to consider\
            \ at every split\n        max_features = ['auto', 'sqrt']\n        # Maximum\
            \ number of levels in tree\n        max_depth = [int(x) for x in np.linspace(10,\
            \ 110, num = 11)]\n        max_depth.append(None)\n        # Minimum number\
            \ of samples required to split a node\n        min_samples_split = [2,\
            \ 5, 10]\n        # Minimum number of samples required at each leaf node\n\
            \        min_samples_leaf = [1, 2, 4]\n        # Method of selecting samples\
            \ for training each tree\n        bootstrap = [True, False]# Create the\
            \ random grid\n        params = {'n_estimators': n_estimators,\n     \
            \               'max_features': ['sqrt'],\n                    'max_depth':\
            \ [20,50,70],\n                    'bootstrap': [True],\n            \
            \        'n_jobs':[-1]}\n        # Random search based on the grid of\
            \ params and n_iter controls number of random combinations it will try\n\
            \        # n_jobs=-1 means using all processors\n        # random_state\
            \ sets the seed for manner of reproducibility \n        params_search\
            \ = RandomizedSearchCV(rf, params, verbose=1, cv=10, n_iter=10, random_state=seed,\
            \ n_jobs=-1)\n        params_search.fit(x_train,y_train)\n        # Check\
            \ the results from the parameter search  \n        print(params_search.best_score_)\n\
            \        print(params_search.best_params_)\n        print(params_search.best_estimator_)\n\
            \        return params_search.best_estimator_\n\n    def validate_rf(rf,\
            \ x_test, y_test):\n        # Predict on x_test\n        y_test_predicted\
            \ = rf.predict(x_test)\n        # Measure the rmse\n        rmse = np.sqrt(mean_squared_error(y_test,\
            \ y_test_predicted))\n        # Print error\t\n        #print(\"Predictions\
            \ of soil moisture:\", y_test_predicted)\n        #print(\"Original values\
            \ of soil moisture:\", y_test)\n        print(\"The rmse for the validation\
            \ is:\", rmse)\n\n    print(\"Checking if model \", out_model, \" exists\"\
            )\n    if os.path.isfile(out_model):\n        print(\"The model \", out_model,\
            \ \" exists\")\n        return out_model\n    else:\n        # Start by\
            \ reading the data\n        print(\"Reading training data from\", data_path)\n\
            \        # Open and reads file \"data\"\n        with open(data_path)\
            \ as data_file:\n            data = json.load(data_file)\n\n        data\
            \ = json.loads(data)\n\n        x_train = data['x_train']\n        y_train\
            \ = data['y_train']\n        x_test = data['x_test']\n        y_test =\
            \ data['y_test']\n        #print(training_data)\n\n        # Define initial\
            \ model\n        rf = RandomForestRegressor()\n        # Random parameter\
            \ search for rf\n        best_rf = random_parameter_search(rf, x_train,\
            \ y_train, maxtree, seed)\n        best_rf.fit(x_train, y_train)\n   \
            \     #Path(args.pathtomodel).parent.mkdir(parents=True, exist_ok=True)\n\
            \        pickle.dump(best_rf, open(out_model, 'wb'))\n        # Validate\n\
            \        validate_rf(best_rf, x_test, y_test)\n        return out_model\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf\
            \ train', description='')\n_parser.add_argument(\"--data-path\", dest=\"\
            data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --maxtree\", dest=\"maxtree\", type=int, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--seed\", dest=\"seed\", type=int, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-model\", dest=\"\
            out_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = rf_train(**_parsed_args)\n\n_outputs = [_outputs]\n\
            \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor\
            \ idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: load-data-data
        - name: pvc-train-name
        - name: random_seed
        - name: trees
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf train", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Rf train@sha256=aa9cb499f8476811cab1240a25243f7446e226c2cf204bafdc6b5c240a68589c"}'
      retries: 3
    - name: knn-inference
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0000.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0000.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0000.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0000.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0000.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0000.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0001.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0001.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0001.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0001.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0001.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0001.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0002.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0002.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0002.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0002.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0002.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0002.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-4
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0003.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0003.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0003.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-4
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0003.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0003.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0003.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-5
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0004.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0004.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0004.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-5
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0004.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0004.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0004.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-6
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0005.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0005.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0005.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-6
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0005.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0005.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0005.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-7
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0006.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0006.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0006.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-7
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0006.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0006.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0006.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-8
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0007.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0007.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0007.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-8
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0007.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0007.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0007.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-9
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0008.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0008.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0008.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-Output
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-9
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-scaler
        value: $(tasks.load-data.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-Output
        value: $(tasks.rf-train.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-Output)
          - --scaler-path
          - $(inputs.params.load-data-scaler)
          - --eval-path
          - /eval/eval_tile_0008.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0008.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0008.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: load-data-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-path
          - /train/2010_03.tif
          - --dir
          - /train/
          - --out-data
          - /train/2010_03.json
          - '----output-paths'
          - $(results.data.path)
          - $(results.scaler.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def load_data(input_path,dir, out_data):
                import numpy as np
                import json
                import pandas as pd
                import pickle
                from sklearn.preprocessing import StandardScaler
                from sklearn.model_selection import train_test_split
                from osgeo import gdal

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                            b = ds.GetRasterBand(band + 1)
                            names.append(b.GetDescription())
                    ds = None
                    return names

                def tif2df(raster_file):
                    ds = gdal.Open(raster_file, 0)
                    xmin, res, _, ymax, _, _ = ds.GetGeoTransform()
                    xsize = ds.RasterXSize
                    ysize = ds.RasterYSize
                    xstart = xmin + res / 2
                    ystart = ymax - res / 2

                    x = np.arange(xstart, xstart + xsize * res, res)
                    y = np.arange(ystart, ystart - ysize * res, -res)
                    x = np.tile(x[:xsize], ysize)
                    y = np.repeat(y[:ysize], xsize)

                    band_names = get_band_names(raster_file)

                    n_bands = ds.RasterCount
                    bands = np.zeros((x.shape[0], n_bands))
                    for k in range(1, n_bands + 1):
                            band = ds.GetRasterBand(k)
                            data = band.ReadAsArray()
                            data = np.ma.array(data, mask=np.equal(data, band.GetNoDataValue()))
                            data = data.filled(np.nan)
                            bands[:, k-1] = data.flatten()

                    column_names = ['x', 'y'] + band_names
                    stack = np.column_stack((x, y, bands))
                    df = pd.DataFrame(stack, columns=column_names)
                    df.dropna(inplace=True)
                    # df.to_csv(output_file, index=None)
                    return df

                print("Reading training data from", input_path)
                training_data = tif2df(input_path)
                print(training_data)

                x_train, x_test, y_train, y_test = train_test_split(training_data.loc[:,training_data.columns != 'z'], training_data.loc[:,'z'], test_size=0.1)
                #print(x_train,"\n",y_train,"TEST\n",x_test,y_test)
                ss = StandardScaler()
                x_train = ss.fit_transform(x_train)
                x_test = ss.transform(x_test)
                #print("SCALED")
                #print(x_train,"\n",y_train,"TEST\n",x_test,y_test)

                # Save scaler model so it can be reused for predicting
                pickle.dump(ss, open(dir+'scaler.pkl', 'wb'))

                # Save data to train with different ml-models
                data = {'x_train' : x_train.tolist(),
                        'y_train' : y_train.tolist(),
                        'x_test' : x_test.tolist(),
                        'y_test' : y_test.tolist()}
                # Creates a json object based on `data`
                data_json = json.dumps(data)

                # Saves the json object into a file
                with open(out_data, 'w') as out_file:
                    json.dump(data_json, out_file)

                   # Create path to where data and scaler are saved

                return [out_data,dir+'scaler.pkl']

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Load data', description='')
            _parser.add_argument("--input-path", dest="input_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--out-data", dest="out_data", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = load_data(**_parsed_args)

            _output_serializers = [
                _serialize_str,
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: pvc-train-name
        results:
        - name: data
          type: string
          description: /tmp/outputs/data/data
        - name: scaler
          type: string
          description: /tmp/outputs/scaler/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Load data", "outputs":
              [{"name": "data", "type": "String"}, {"name": "scaler", "type": "String"}],
              "version": "Load data@sha256=b95e43bb0c0e533f62c81cde1dac345ef7c794440c1eb046738dc9e11e4ff3b5"}'
      retries: 3
    - name: knn-train-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: k_neighbors
        value: $(params.k_neighbors)
      - name: load-data-2-data
        value: $(tasks.load-data-2.results.data)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: random_seed
        value: $(params.random_seed)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - $(inputs.params.load-data-2-data)
          - --k
          - $(inputs.params.k_neighbors)
          - --seed
          - $(inputs.params.random_seed)
          - --out-model
          - /train/model_knn2.pkl
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_train (data_path, k, seed, out_model):\n    import numpy as np\n\
            \    import pickle\n    import json\n    from sklearn.neighbors import\
            \ KNeighborsRegressor\n    from sklearn.model_selection import RandomizedSearchCV\n\
            \    from sklearn.metrics import mean_squared_error\n    import os\n\n\
            \    # Define functions for training\n    def random_parameter_search(knn,\
            \ x_train, y_train, maxK, seed):\n        # Dictionary with all the hyperparameter\
            \ options for the knn model: n_neighbors, weights, metric\n        params\
            \ = {'n_neighbors': list(range(2,maxK)),\n            'weights': ['uniform','distance',\
            \ gaussian],\n            'metric': ['euclidean','minkowski']\n      \
            \          }\n        # Random search based on the grid of params and\
            \ n_iter controls number of random combinations it will try\n        #\
            \ n_jobs=-1 means using all processors\n        # random_state sets the\
            \ seed for manner of reproducibility \n        params_search = RandomizedSearchCV(knn,\
            \ params, verbose=1, cv=10, n_iter=50, random_state=seed, n_jobs=-1)\n\
            \        params_search.fit(x_train,y_train)\n        # Check the results\
            \ from the parameter search  \n        print(params_search.best_score_)\n\
            \        print(params_search.best_params_)\n        print(params_search.best_estimator_)\n\
            \        return params_search.best_params_\n\n    def gaussian(dist, sigma\
            \ = 4):\n        # Input a distance and return its weight using the gaussian\
            \ kernel \n        weight = np.exp(-dist**2/(2*sigma**2))\n        return\
            \ weight\n\n    def validate_knn(knn, x_test, y_test):\n        # Predict\
            \ on x_test\n        y_test_predicted = knn.predict(x_test)\n        #\
            \ Measure the rmse\n        rmse = np.sqrt(mean_squared_error(y_test,\
            \ y_test_predicted))\n        # Print error\t\n        #print(\"Predictions\
            \ of soil moisture:\", y_test_predicted)\n        #print(\"Original values\
            \ of soil moisture:\", y_test)\n        print(\"The rmse for the validation\
            \ is:\", rmse)\n\n    print(\"Checking if model \", out_model, \" exists\"\
            )\n    if os.path.isfile(out_model):\n        print(\"The model \", out_model,\
            \ \" exists\")\n        return out_model\n    else:\n        print(\"\
            Reading training data from\", data_path)\n            # Open and reads\
            \ file \"data\"\n        with open(data_path) as data_file:\n        \
            \    data = json.load(data_file)\n\n        data = json.loads(data)\n\n\
            \        x_train = data['x_train']\n        y_train = data['y_train']\n\
            \        x_test = data['x_test']\n        y_test = data['y_test']\n  \
            \      #print(training_data)\n        maxK = int(k)\n        seed = int(seed)\n\
            \n            # Define initial model\n        knn = KNeighborsRegressor(leaf_size=3000)\n\
            \        # Random parameter search of n_neighbors, weigths and metric\n\
            \        best_params = random_parameter_search(knn, x_train, y_train,\
            \ maxK, seed)\n        # Based on selection build the new regressor\n\
            \        knn = KNeighborsRegressor(n_neighbors=best_params['n_neighbors'],\
            \ weights=best_params['weights'],\n                        metric=best_params['metric'],\
            \ n_jobs=-1)\n        # Fit the new model to data\n        knn.fit(x_train,\
            \ y_train)\n        # Save model\n        pickle.dump(knn, open(out_model,\
            \ 'wb'))\n\n        # Validate\n        validate_knn(knn, x_test, y_test)\n\
            \        return out_model\n\ndef _serialize_str(str_value: str) -> str:\n\
            \    if not isinstance(str_value, str):\n        raise TypeError('Value\
            \ \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value),\
            \ str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser\
            \ = argparse.ArgumentParser(prog='Knn train', description='')\n_parser.add_argument(\"\
            --data-path\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--k\", dest=\"k\", type=int, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--seed\", dest=\"seed\", type=int, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-model\", dest=\"\
            out_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = knn_train(**_parsed_args)\n\n_outputs = [_outputs]\n\
            \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor\
            \ idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: k_neighbors
        - name: load-data-2-data
        - name: pvc-train-name
        - name: random_seed
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn train", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Knn train@sha256=2d6f1c535600b8f4370aa2afe20e9b3e914423a0b861ea009a5434a4de95316d"}'
      retries: 3
    - name: rf-train-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-data
        value: $(tasks.load-data-2.results.data)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: random_seed
        value: $(params.random_seed)
      - name: trees
        value: $(params.trees)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - $(inputs.params.load-data-2-data)
          - --maxtree
          - $(inputs.params.trees)
          - --seed
          - $(inputs.params.random_seed)
          - --out-model
          - /train/model_rf2.pkl
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_train(data_path, maxtree, seed, out_model):\n    # Libraries\n\
            \    import numpy as np\n    import json\n    import pickle\n    from\
            \ sklearn.ensemble import RandomForestRegressor\n    from sklearn.model_selection\
            \ import RandomizedSearchCV\n    from sklearn.metrics import mean_squared_error\n\
            \    import os\n\n    # Functions \n    def random_parameter_search(rf,\
            \ x_train, y_train, maxtree, seed):\n        # Number of trees in random\
            \ forest\n        n_estimators = [int(x) for x in np.linspace(start =\
            \ 300, stop = maxtree, num = 100)]\n        # Number of features to consider\
            \ at every split\n        max_features = ['auto', 'sqrt']\n        # Maximum\
            \ number of levels in tree\n        max_depth = [int(x) for x in np.linspace(10,\
            \ 110, num = 11)]\n        max_depth.append(None)\n        # Minimum number\
            \ of samples required to split a node\n        min_samples_split = [2,\
            \ 5, 10]\n        # Minimum number of samples required at each leaf node\n\
            \        min_samples_leaf = [1, 2, 4]\n        # Method of selecting samples\
            \ for training each tree\n        bootstrap = [True, False]# Create the\
            \ random grid\n        params = {'n_estimators': n_estimators,\n     \
            \               'max_features': ['sqrt'],\n                    'max_depth':\
            \ [20,50,70],\n                    'bootstrap': [True],\n            \
            \        'n_jobs':[-1]}\n        # Random search based on the grid of\
            \ params and n_iter controls number of random combinations it will try\n\
            \        # n_jobs=-1 means using all processors\n        # random_state\
            \ sets the seed for manner of reproducibility \n        params_search\
            \ = RandomizedSearchCV(rf, params, verbose=1, cv=10, n_iter=10, random_state=seed,\
            \ n_jobs=-1)\n        params_search.fit(x_train,y_train)\n        # Check\
            \ the results from the parameter search  \n        print(params_search.best_score_)\n\
            \        print(params_search.best_params_)\n        print(params_search.best_estimator_)\n\
            \        return params_search.best_estimator_\n\n    def validate_rf(rf,\
            \ x_test, y_test):\n        # Predict on x_test\n        y_test_predicted\
            \ = rf.predict(x_test)\n        # Measure the rmse\n        rmse = np.sqrt(mean_squared_error(y_test,\
            \ y_test_predicted))\n        # Print error\t\n        #print(\"Predictions\
            \ of soil moisture:\", y_test_predicted)\n        #print(\"Original values\
            \ of soil moisture:\", y_test)\n        print(\"The rmse for the validation\
            \ is:\", rmse)\n\n    print(\"Checking if model \", out_model, \" exists\"\
            )\n    if os.path.isfile(out_model):\n        print(\"The model \", out_model,\
            \ \" exists\")\n        return out_model\n    else:\n        # Start by\
            \ reading the data\n        print(\"Reading training data from\", data_path)\n\
            \        # Open and reads file \"data\"\n        with open(data_path)\
            \ as data_file:\n            data = json.load(data_file)\n\n        data\
            \ = json.loads(data)\n\n        x_train = data['x_train']\n        y_train\
            \ = data['y_train']\n        x_test = data['x_test']\n        y_test =\
            \ data['y_test']\n        #print(training_data)\n\n        # Define initial\
            \ model\n        rf = RandomForestRegressor()\n        # Random parameter\
            \ search for rf\n        best_rf = random_parameter_search(rf, x_train,\
            \ y_train, maxtree, seed)\n        best_rf.fit(x_train, y_train)\n   \
            \     #Path(args.pathtomodel).parent.mkdir(parents=True, exist_ok=True)\n\
            \        pickle.dump(best_rf, open(out_model, 'wb'))\n        # Validate\n\
            \        validate_rf(best_rf, x_test, y_test)\n        return out_model\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf\
            \ train', description='')\n_parser.add_argument(\"--data-path\", dest=\"\
            data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --maxtree\", dest=\"maxtree\", type=int, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--seed\", dest=\"seed\", type=int, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-model\", dest=\"\
            out_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = rf_train(**_parsed_args)\n\n_outputs = [_outputs]\n\
            \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor\
            \ idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: load-data-2-data
        - name: pvc-train-name
        - name: random_seed
        - name: trees
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf train", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Rf train@sha256=aa9cb499f8476811cab1240a25243f7446e226c2cf204bafdc6b5c240a68589c"}'
      retries: 3
    - name: knn-inference-10
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0000.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0000.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0000.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-10
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0000.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0000.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0000.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-11
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0001.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0001.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0001.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-11
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0001.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0001.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0001.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-12
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0002.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0002.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0002.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-12
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0002.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0002.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0002.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-13
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0003.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0003.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0003.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-13
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0003.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0003.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0003.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-14
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0004.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0004.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0004.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-14
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0004.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0004.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0004.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-15
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0005.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0005.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0005.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-15
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0005.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0005.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0005.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-16
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0006.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0006.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0006.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-16
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0006.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0006.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0006.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-17
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0007.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0007.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0007.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-17
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0007.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0007.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0007.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-18
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-2-Output
        value: $(tasks.knn-train-2.results.Output)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0008.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0008.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0008.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-2-Output
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-18
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-2-scaler
        value: $(tasks.load-data-2.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-2-Output
        value: $(tasks.rf-train-2.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-2-Output)
          - --scaler-path
          - $(inputs.params.load-data-2-scaler)
          - --eval-path
          - /eval/eval_tile_0008.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0008.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0008.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-2-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-2-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: load-data-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-path
          - /train/2010_04.tif
          - --dir
          - /train/
          - --out-data
          - /train/2010_04.json
          - '----output-paths'
          - $(results.data.path)
          - $(results.scaler.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def load_data(input_path,dir, out_data):
                import numpy as np
                import json
                import pandas as pd
                import pickle
                from sklearn.preprocessing import StandardScaler
                from sklearn.model_selection import train_test_split
                from osgeo import gdal

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                            b = ds.GetRasterBand(band + 1)
                            names.append(b.GetDescription())
                    ds = None
                    return names

                def tif2df(raster_file):
                    ds = gdal.Open(raster_file, 0)
                    xmin, res, _, ymax, _, _ = ds.GetGeoTransform()
                    xsize = ds.RasterXSize
                    ysize = ds.RasterYSize
                    xstart = xmin + res / 2
                    ystart = ymax - res / 2

                    x = np.arange(xstart, xstart + xsize * res, res)
                    y = np.arange(ystart, ystart - ysize * res, -res)
                    x = np.tile(x[:xsize], ysize)
                    y = np.repeat(y[:ysize], xsize)

                    band_names = get_band_names(raster_file)

                    n_bands = ds.RasterCount
                    bands = np.zeros((x.shape[0], n_bands))
                    for k in range(1, n_bands + 1):
                            band = ds.GetRasterBand(k)
                            data = band.ReadAsArray()
                            data = np.ma.array(data, mask=np.equal(data, band.GetNoDataValue()))
                            data = data.filled(np.nan)
                            bands[:, k-1] = data.flatten()

                    column_names = ['x', 'y'] + band_names
                    stack = np.column_stack((x, y, bands))
                    df = pd.DataFrame(stack, columns=column_names)
                    df.dropna(inplace=True)
                    # df.to_csv(output_file, index=None)
                    return df

                print("Reading training data from", input_path)
                training_data = tif2df(input_path)
                print(training_data)

                x_train, x_test, y_train, y_test = train_test_split(training_data.loc[:,training_data.columns != 'z'], training_data.loc[:,'z'], test_size=0.1)
                #print(x_train,"\n",y_train,"TEST\n",x_test,y_test)
                ss = StandardScaler()
                x_train = ss.fit_transform(x_train)
                x_test = ss.transform(x_test)
                #print("SCALED")
                #print(x_train,"\n",y_train,"TEST\n",x_test,y_test)

                # Save scaler model so it can be reused for predicting
                pickle.dump(ss, open(dir+'scaler.pkl', 'wb'))

                # Save data to train with different ml-models
                data = {'x_train' : x_train.tolist(),
                        'y_train' : y_train.tolist(),
                        'x_test' : x_test.tolist(),
                        'y_test' : y_test.tolist()}
                # Creates a json object based on `data`
                data_json = json.dumps(data)

                # Saves the json object into a file
                with open(out_data, 'w') as out_file:
                    json.dump(data_json, out_file)

                   # Create path to where data and scaler are saved

                return [out_data,dir+'scaler.pkl']

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Load data', description='')
            _parser.add_argument("--input-path", dest="input_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--out-data", dest="out_data", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = load_data(**_parsed_args)

            _output_serializers = [
                _serialize_str,
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: pvc-train-name
        results:
        - name: data
          type: string
          description: /tmp/outputs/data/data
        - name: scaler
          type: string
          description: /tmp/outputs/scaler/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Load data", "outputs":
              [{"name": "data", "type": "String"}, {"name": "scaler", "type": "String"}],
              "version": "Load data@sha256=b95e43bb0c0e533f62c81cde1dac345ef7c794440c1eb046738dc9e11e4ff3b5"}'
      retries: 3
    - name: knn-train-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: k_neighbors
        value: $(params.k_neighbors)
      - name: load-data-3-data
        value: $(tasks.load-data-3.results.data)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: random_seed
        value: $(params.random_seed)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - $(inputs.params.load-data-3-data)
          - --k
          - $(inputs.params.k_neighbors)
          - --seed
          - $(inputs.params.random_seed)
          - --out-model
          - /train/model_knn3.pkl
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_train (data_path, k, seed, out_model):\n    import numpy as np\n\
            \    import pickle\n    import json\n    from sklearn.neighbors import\
            \ KNeighborsRegressor\n    from sklearn.model_selection import RandomizedSearchCV\n\
            \    from sklearn.metrics import mean_squared_error\n    import os\n\n\
            \    # Define functions for training\n    def random_parameter_search(knn,\
            \ x_train, y_train, maxK, seed):\n        # Dictionary with all the hyperparameter\
            \ options for the knn model: n_neighbors, weights, metric\n        params\
            \ = {'n_neighbors': list(range(2,maxK)),\n            'weights': ['uniform','distance',\
            \ gaussian],\n            'metric': ['euclidean','minkowski']\n      \
            \          }\n        # Random search based on the grid of params and\
            \ n_iter controls number of random combinations it will try\n        #\
            \ n_jobs=-1 means using all processors\n        # random_state sets the\
            \ seed for manner of reproducibility \n        params_search = RandomizedSearchCV(knn,\
            \ params, verbose=1, cv=10, n_iter=50, random_state=seed, n_jobs=-1)\n\
            \        params_search.fit(x_train,y_train)\n        # Check the results\
            \ from the parameter search  \n        print(params_search.best_score_)\n\
            \        print(params_search.best_params_)\n        print(params_search.best_estimator_)\n\
            \        return params_search.best_params_\n\n    def gaussian(dist, sigma\
            \ = 4):\n        # Input a distance and return its weight using the gaussian\
            \ kernel \n        weight = np.exp(-dist**2/(2*sigma**2))\n        return\
            \ weight\n\n    def validate_knn(knn, x_test, y_test):\n        # Predict\
            \ on x_test\n        y_test_predicted = knn.predict(x_test)\n        #\
            \ Measure the rmse\n        rmse = np.sqrt(mean_squared_error(y_test,\
            \ y_test_predicted))\n        # Print error\t\n        #print(\"Predictions\
            \ of soil moisture:\", y_test_predicted)\n        #print(\"Original values\
            \ of soil moisture:\", y_test)\n        print(\"The rmse for the validation\
            \ is:\", rmse)\n\n    print(\"Checking if model \", out_model, \" exists\"\
            )\n    if os.path.isfile(out_model):\n        print(\"The model \", out_model,\
            \ \" exists\")\n        return out_model\n    else:\n        print(\"\
            Reading training data from\", data_path)\n            # Open and reads\
            \ file \"data\"\n        with open(data_path) as data_file:\n        \
            \    data = json.load(data_file)\n\n        data = json.loads(data)\n\n\
            \        x_train = data['x_train']\n        y_train = data['y_train']\n\
            \        x_test = data['x_test']\n        y_test = data['y_test']\n  \
            \      #print(training_data)\n        maxK = int(k)\n        seed = int(seed)\n\
            \n            # Define initial model\n        knn = KNeighborsRegressor(leaf_size=3000)\n\
            \        # Random parameter search of n_neighbors, weigths and metric\n\
            \        best_params = random_parameter_search(knn, x_train, y_train,\
            \ maxK, seed)\n        # Based on selection build the new regressor\n\
            \        knn = KNeighborsRegressor(n_neighbors=best_params['n_neighbors'],\
            \ weights=best_params['weights'],\n                        metric=best_params['metric'],\
            \ n_jobs=-1)\n        # Fit the new model to data\n        knn.fit(x_train,\
            \ y_train)\n        # Save model\n        pickle.dump(knn, open(out_model,\
            \ 'wb'))\n\n        # Validate\n        validate_knn(knn, x_test, y_test)\n\
            \        return out_model\n\ndef _serialize_str(str_value: str) -> str:\n\
            \    if not isinstance(str_value, str):\n        raise TypeError('Value\
            \ \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value),\
            \ str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser\
            \ = argparse.ArgumentParser(prog='Knn train', description='')\n_parser.add_argument(\"\
            --data-path\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--k\", dest=\"k\", type=int, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--seed\", dest=\"seed\", type=int, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-model\", dest=\"\
            out_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = knn_train(**_parsed_args)\n\n_outputs = [_outputs]\n\
            \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor\
            \ idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: k_neighbors
        - name: load-data-3-data
        - name: pvc-train-name
        - name: random_seed
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn train", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Knn train@sha256=2d6f1c535600b8f4370aa2afe20e9b3e914423a0b861ea009a5434a4de95316d"}'
      retries: 3
    - name: rf-train-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-data
        value: $(tasks.load-data-3.results.data)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: random_seed
        value: $(params.random_seed)
      - name: trees
        value: $(params.trees)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - $(inputs.params.load-data-3-data)
          - --maxtree
          - $(inputs.params.trees)
          - --seed
          - $(inputs.params.random_seed)
          - --out-model
          - /train/model_rf3.pkl
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_train(data_path, maxtree, seed, out_model):\n    # Libraries\n\
            \    import numpy as np\n    import json\n    import pickle\n    from\
            \ sklearn.ensemble import RandomForestRegressor\n    from sklearn.model_selection\
            \ import RandomizedSearchCV\n    from sklearn.metrics import mean_squared_error\n\
            \    import os\n\n    # Functions \n    def random_parameter_search(rf,\
            \ x_train, y_train, maxtree, seed):\n        # Number of trees in random\
            \ forest\n        n_estimators = [int(x) for x in np.linspace(start =\
            \ 300, stop = maxtree, num = 100)]\n        # Number of features to consider\
            \ at every split\n        max_features = ['auto', 'sqrt']\n        # Maximum\
            \ number of levels in tree\n        max_depth = [int(x) for x in np.linspace(10,\
            \ 110, num = 11)]\n        max_depth.append(None)\n        # Minimum number\
            \ of samples required to split a node\n        min_samples_split = [2,\
            \ 5, 10]\n        # Minimum number of samples required at each leaf node\n\
            \        min_samples_leaf = [1, 2, 4]\n        # Method of selecting samples\
            \ for training each tree\n        bootstrap = [True, False]# Create the\
            \ random grid\n        params = {'n_estimators': n_estimators,\n     \
            \               'max_features': ['sqrt'],\n                    'max_depth':\
            \ [20,50,70],\n                    'bootstrap': [True],\n            \
            \        'n_jobs':[-1]}\n        # Random search based on the grid of\
            \ params and n_iter controls number of random combinations it will try\n\
            \        # n_jobs=-1 means using all processors\n        # random_state\
            \ sets the seed for manner of reproducibility \n        params_search\
            \ = RandomizedSearchCV(rf, params, verbose=1, cv=10, n_iter=10, random_state=seed,\
            \ n_jobs=-1)\n        params_search.fit(x_train,y_train)\n        # Check\
            \ the results from the parameter search  \n        print(params_search.best_score_)\n\
            \        print(params_search.best_params_)\n        print(params_search.best_estimator_)\n\
            \        return params_search.best_estimator_\n\n    def validate_rf(rf,\
            \ x_test, y_test):\n        # Predict on x_test\n        y_test_predicted\
            \ = rf.predict(x_test)\n        # Measure the rmse\n        rmse = np.sqrt(mean_squared_error(y_test,\
            \ y_test_predicted))\n        # Print error\t\n        #print(\"Predictions\
            \ of soil moisture:\", y_test_predicted)\n        #print(\"Original values\
            \ of soil moisture:\", y_test)\n        print(\"The rmse for the validation\
            \ is:\", rmse)\n\n    print(\"Checking if model \", out_model, \" exists\"\
            )\n    if os.path.isfile(out_model):\n        print(\"The model \", out_model,\
            \ \" exists\")\n        return out_model\n    else:\n        # Start by\
            \ reading the data\n        print(\"Reading training data from\", data_path)\n\
            \        # Open and reads file \"data\"\n        with open(data_path)\
            \ as data_file:\n            data = json.load(data_file)\n\n        data\
            \ = json.loads(data)\n\n        x_train = data['x_train']\n        y_train\
            \ = data['y_train']\n        x_test = data['x_test']\n        y_test =\
            \ data['y_test']\n        #print(training_data)\n\n        # Define initial\
            \ model\n        rf = RandomForestRegressor()\n        # Random parameter\
            \ search for rf\n        best_rf = random_parameter_search(rf, x_train,\
            \ y_train, maxtree, seed)\n        best_rf.fit(x_train, y_train)\n   \
            \     #Path(args.pathtomodel).parent.mkdir(parents=True, exist_ok=True)\n\
            \        pickle.dump(best_rf, open(out_model, 'wb'))\n        # Validate\n\
            \        validate_rf(best_rf, x_test, y_test)\n        return out_model\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf\
            \ train', description='')\n_parser.add_argument(\"--data-path\", dest=\"\
            data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --maxtree\", dest=\"maxtree\", type=int, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--seed\", dest=\"seed\", type=int, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-model\", dest=\"\
            out_model\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = rf_train(**_parsed_args)\n\n_outputs = [_outputs]\n\
            \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor\
            \ idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: load-data-3-data
        - name: pvc-train-name
        - name: random_seed
        - name: trees
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf train", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Rf train@sha256=aa9cb499f8476811cab1240a25243f7446e226c2cf204bafdc6b5c240a68589c"}'
      retries: 3
    - name: knn-inference-19
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0000.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0000.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0000.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-19
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0000.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0000.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0000.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-20
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0001.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0001.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0001.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-20
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0001.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0001.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0001.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-21
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0002.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0002.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0002.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-21
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0002.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0002.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0002.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-22
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0003.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0003.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0003.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-22
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0003.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0003.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0003.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-23
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0004.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0004.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0004.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-23
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0004.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0004.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0004.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-24
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0005.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0005.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0005.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-24
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0005.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0005.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0005.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-25
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0006.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0006.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0006.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-25
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0006.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0006.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0006.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-26
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0007.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0007.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0007.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-26
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0007.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0007.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0007.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
    - name: knn-inference-27
      params:
      - name: container_image
        value: $(params.container_image)
      - name: knn-train-3-Output
        value: $(tasks.knn-train-3.results.Output)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.knn-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0008.tif
          - --out-dir
          - /out_knn/knn/
          - --predictions
          - /out_knn/knn/eval_tile_0008.tif
          - --tmp-pred
          - /out_knn/knn/eval_tile_0008.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (model_path, scaler_path, eval_path,  out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+'predictions.csv',\
            \ model_path)\n    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+'predictions.csv',\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+'predictions.csv')\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ inference', description='')\n_parser.add_argument(\"--model-path\",\
            \ dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--scaler-path\", dest=\"scaler_path\", type=str,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--eval-path\"\
            , dest=\"eval_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--out-dir\", dest=\"out_dir\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--predictions\",\
            \ dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_knn/
            name: pvc-predictions
        params:
        - name: container_image
        - name: knn-train-3-Output
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Knn inference@sha256=b068d199c8dc2d0927e8a69d7c39e6d8a22cdebb361dfb56da64c11833057c3e"}'
      retries: 3
    - name: rf-inference-27
      params:
      - name: container_image
        value: $(params.container_image)
      - name: load-data-3-scaler
        value: $(tasks.load-data-3.results.scaler)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-predictions-name
        value: $(tasks.pvc-predictions.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      - name: rf-train-3-Output
        value: $(tasks.rf-train-3.results.Output)
      taskSpec:
        steps:
        - name: main
          args:
          - --model-path
          - $(inputs.params.rf-train-3-Output)
          - --scaler-path
          - $(inputs.params.load-data-3-scaler)
          - --eval-path
          - /eval/eval_tile_0008.tif
          - --out-dir
          - /out_rf/rf/
          - --predictions
          - /out_rf/rf/eval_tile_0008.tif
          - --tmp-pred
          - /out_rf/rf/eval_tile_0008.csv
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def rf_inference(model_path, scaler_path, eval_path, out_dir, predictions,\
            \ tmp_pred):\n    import numpy as np\n    import pickle  \n    from osgeo\
            \ import gdal, ogr  \n    import sklearn\n    import os\n\n    def get_band_names(raster):\n\
            \        ds = gdal.Open(raster, 0)\n        names = []\n        for band\
            \ in range(ds.RasterCount):\n            b = ds.GetRasterBand(band + 1)\n\
            \            names.append(b.GetDescription())\n        ds = None\n   \
            \     return names\n\n    def load_ds(evaluation_file, scaler_file):\n\
            \        evaluation_data = tif2arr(evaluation_file) \n        ss = pickle.load(open(scaler_file,\
            \ 'rb'))\n        x_predict = ss.transform(evaluation_data)\n        evaluation_data\
            \ = evaluation_data[:,0:2]\n        return x_predict, evaluation_data\n\
            \n    def tif2df(raster_file, band_names) :\n        ds = gdal.Open(raster_file,\
            \ 0)\n        xmin, res, _, ymax, _, _ = ds.GetGeoTransform()\n      \
            \  xsize = ds.RasterXSize\n        ysize = ds.RasterYSize\n        xstart\
            \ = xmin + res / 2\n        ystart = ymax - res / 2\n\n        x = np.arange(xstart,\
            \ xstart + xsize * res, res, dtype=np.single)\n        y = np.arange(ystart,\
            \ ystart - ysize * res, -res,dtype=np.single)\n        x = np.tile(x[:xsize],\
            \ ysize)\n        y = np.repeat(y[:ysize], xsize)\n\n        n_bands =\
            \ ds.RasterCount\n        data = np.zeros((x.shape[0], n_bands), dtype=np.single)\n\
            \        for k in range(1, n_bands + 1):\n            band = ds.GetRasterBand(k)\n\
            \            data[:, k-1] = band.ReadAsArray().flatten().astype(np.single)\n\
            \n        data = np.column_stack((x, y, data))\n        del x, y\n   \
            \     data = data[~np.isnan(data).any(axis=1)]\n        return data\n\n\
            \    def rasterize(input_file, output_file, xres, yres):\n        # When\
            \ there is not a regular grid (has missing values)\n        vrt_file =\
            \ output_file[:-4] + '.vrt'\n        if os.path.exists(vrt_file):\n  \
            \          os.remove(vrt_file)\n\n        f = open(vrt_file, 'w')\n  \
            \      f.write('<OGRVRTDataSource>\\n \\\n        <OGRVRTLayer name=\"\
            {}\">\\n \\\n            <SrcDataSource>{}</SrcDataSource>\\n \\\n   \
            \         <GeometryType>wkbPoint</GeometryType>\\n \\\n            <GeometryField\
            \ encoding=\"PointFromColumns\" x=\"x\" y=\"y\" z=\"z\"/>\\n \\\n    \
            \    </OGRVRTLayer>\\n \\\n    </OGRVRTDataSource>'.format('predictions',\
            \ input_file)) # https://gdal.org/programs/gdal_grid.html#gdal-grid\n\
            \        f.close()\n\n        rasterize_options = gdal.RasterizeOptions(xRes=xres,\
            \ yRes=yres, attribute='z', noData=np.nan, outputType=gdal.GDT_Float32,\
            \ creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)\n\
            \        r = gdal.Rasterize(output_file, vrt_file, options=rasterize_options)\n\
            \        r = None\n        os.remove(vrt_file)\n\n    def predict(x_predict,\
            \ evaluation_data, out_file, model_file):\n        model = pickle.load(open(model_file,\
            \ 'rb'))\n        # Predict on evaluation data\n        y_predict = model.predict(x_predict)\n\
            \n        evaluation_data = np.column_stack((evaluation_data, y_predict))\n\
            \        print(\"DATA SHAPE: \", evaluation_data.shape)\n        np.savetxt(out_file,\
            \ evaluation_data, fmt='%.7f', header='x,y,z', delimiter=',', comments='')\n\
            \n    x_predict, evaluation_data = load_ds(eval_path, scaler_file)\n \
            \   band_names = get_band_names(evaluation_file)\n    print(\"Band names:\
            \ \", band_names)\n\n    print(\"Running model to get predictions...\"\
            )\n    predict(x_predict, evaluation_data, out_dir+tmp_pred, model_path)\n\
            \    ds = gdal.Open(evaluation_file)\n    gt = ds.GetGeoTransform()\n\
            \    print(\"Running rasterize...\")\n    rasterize(out_dir+tmp_pred,\
            \ predictions, gt[1], gt[5])\n    os.remove(out_dir+tmp_pred)\n\ndef _serialize_str(str_value:\
            \ str) -> str:\n    if not isinstance(str_value, str):\n        raise\
            \ TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(\n \
            \           str(str_value), str(type(str_value))))\n    return str_value\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Rf inference',\
            \ description='')\n_parser.add_argument(\"--model-path\", dest=\"model_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --scaler-path\", dest=\"scaler_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--eval-path\", dest=\"eval_path\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--out-dir\", dest=\"\
            out_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --predictions\", dest=\"predictions\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--tmp-pred\", dest=\"tmp_pred\", type=str, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ rf_inference(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 20G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /out_rf/
            name: pvc-predictions
        params:
        - name: container_image
        - name: load-data-3-scaler
        - name: pvc-eval-name
        - name: pvc-predictions-name
        - name: pvc-train-name
        - name: rf-train-3-Output
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-predictions
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-predictions-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Rf inference",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Rf inference@sha256=84be09cfacd79dda083b6cc74ca977915b00ff2f3cf99d8e66ffc21e50b6cac2"}'
      retries: 3
  taskRunSpecs:
  - pipelineTaskName: knn-inference
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-2
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-2
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-3
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-3
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-4
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-4
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-5
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-5
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-6
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-6
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-7
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-7
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-8
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-8
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-9
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-9
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-10
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-10
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-11
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-11
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-12
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-12
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-13
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-13
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-14
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-14
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-15
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-15
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-16
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-16
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-17
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-17
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-18
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-18
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-19
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-19
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-20
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-20
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-21
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-21
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-22
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-22
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-23
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-23
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-24
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-24
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-25
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-25
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-26
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-26
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: knn-inference-27
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
  - pipelineTaskName: rf-inference-27
    taskPodTemplate:
      nodeSelector:
        ibm-cloud.kubernetes.io/worker-pool-name: inference
