apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: somospie-data-generation-pipeline
  annotations:
    tekton.dev/output_artifacts: '{"build-stack-eval": [{"key": "artifacts/$PIPELINERUN/build-stack-eval/Output.tgz",
      "name": "build-stack-eval-Output", "path": "/tmp/outputs/Output/data"}], "build-stack-train":
      [{"key": "artifacts/$PIPELINERUN/build-stack-train/Output.tgz", "name": "build-stack-train-Output",
      "path": "/tmp/outputs/Output/data"}], "build-stack-train-10": [{"key": "artifacts/$PIPELINERUN/build-stack-train-10/Output.tgz",
      "name": "build-stack-train-10-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-11": [{"key": "artifacts/$PIPELINERUN/build-stack-train-11/Output.tgz",
      "name": "build-stack-train-11-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-12": [{"key": "artifacts/$PIPELINERUN/build-stack-train-12/Output.tgz",
      "name": "build-stack-train-12-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-2": [{"key": "artifacts/$PIPELINERUN/build-stack-train-2/Output.tgz",
      "name": "build-stack-train-2-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-3": [{"key": "artifacts/$PIPELINERUN/build-stack-train-3/Output.tgz",
      "name": "build-stack-train-3-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-4": [{"key": "artifacts/$PIPELINERUN/build-stack-train-4/Output.tgz",
      "name": "build-stack-train-4-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-5": [{"key": "artifacts/$PIPELINERUN/build-stack-train-5/Output.tgz",
      "name": "build-stack-train-5-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-6": [{"key": "artifacts/$PIPELINERUN/build-stack-train-6/Output.tgz",
      "name": "build-stack-train-6-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-7": [{"key": "artifacts/$PIPELINERUN/build-stack-train-7/Output.tgz",
      "name": "build-stack-train-7-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-8": [{"key": "artifacts/$PIPELINERUN/build-stack-train-8/Output.tgz",
      "name": "build-stack-train-8-Output", "path": "/tmp/outputs/Output/data"}],
      "build-stack-train-9": [{"key": "artifacts/$PIPELINERUN/build-stack-train-9/Output.tgz",
      "name": "build-stack-train-9-Output", "path": "/tmp/outputs/Output/data"}],
      "crop-region-eval": [{"key": "artifacts/$PIPELINERUN/crop-region-eval/Output.tgz",
      "name": "crop-region-eval-Output", "path": "/tmp/outputs/Output/data"}], "crop-region-eval-2":
      [{"key": "artifacts/$PIPELINERUN/crop-region-eval-2/Output.tgz", "name": "crop-region-eval-2-Output",
      "path": "/tmp/outputs/Output/data"}], "crop-region-eval-3": [{"key": "artifacts/$PIPELINERUN/crop-region-eval-3/Output.tgz",
      "name": "crop-region-eval-3-Output", "path": "/tmp/outputs/Output/data"}], "crop-region-eval-4":
      [{"key": "artifacts/$PIPELINERUN/crop-region-eval-4/Output.tgz", "name": "crop-region-eval-4-Output",
      "path": "/tmp/outputs/Output/data"}], "crop-tile": [{"key": "artifacts/$PIPELINERUN/crop-tile/Output.tgz",
      "name": "crop-tile-Output", "path": "/tmp/outputs/Output/data"}], "crop-tile-2":
      [{"key": "artifacts/$PIPELINERUN/crop-tile-2/Output.tgz", "name": "crop-tile-2-Output",
      "path": "/tmp/outputs/Output/data"}], "crop-tile-3": [{"key": "artifacts/$PIPELINERUN/crop-tile-3/Output.tgz",
      "name": "crop-tile-3-Output", "path": "/tmp/outputs/Output/data"}], "crop-tile-4":
      [{"key": "artifacts/$PIPELINERUN/crop-tile-4/Output.tgz", "name": "crop-tile-4-Output",
      "path": "/tmp/outputs/Output/data"}], "download-parallel": [{"key": "artifacts/$PIPELINERUN/download-parallel/Output.tgz",
      "name": "download-parallel-Output", "path": "/tmp/outputs/Output/data"}], "get-shp":
      [{"key": "artifacts/$PIPELINERUN/get-shp/Output.tgz", "name": "get-shp-Output",
      "path": "/tmp/outputs/Output/data"}], "merge-avg": [{"key": "artifacts/$PIPELINERUN/merge-avg/Output.tgz",
      "name": "merge-avg-Output", "path": "/tmp/outputs/Output/data"}], "merge-avg-10":
      [{"key": "artifacts/$PIPELINERUN/merge-avg-10/Output.tgz", "name": "merge-avg-10-Output",
      "path": "/tmp/outputs/Output/data"}], "merge-avg-11": [{"key": "artifacts/$PIPELINERUN/merge-avg-11/Output.tgz",
      "name": "merge-avg-11-Output", "path": "/tmp/outputs/Output/data"}], "merge-avg-12":
      [{"key": "artifacts/$PIPELINERUN/merge-avg-12/Output.tgz", "name": "merge-avg-12-Output",
      "path": "/tmp/outputs/Output/data"}], "merge-avg-2": [{"key": "artifacts/$PIPELINERUN/merge-avg-2/Output.tgz",
      "name": "merge-avg-2-Output", "path": "/tmp/outputs/Output/data"}], "merge-avg-3":
      [{"key": "artifacts/$PIPELINERUN/merge-avg-3/Output.tgz", "name": "merge-avg-3-Output",
      "path": "/tmp/outputs/Output/data"}], "merge-avg-4": [{"key": "artifacts/$PIPELINERUN/merge-avg-4/Output.tgz",
      "name": "merge-avg-4-Output", "path": "/tmp/outputs/Output/data"}], "merge-avg-5":
      [{"key": "artifacts/$PIPELINERUN/merge-avg-5/Output.tgz", "name": "merge-avg-5-Output",
      "path": "/tmp/outputs/Output/data"}], "merge-avg-6": [{"key": "artifacts/$PIPELINERUN/merge-avg-6/Output.tgz",
      "name": "merge-avg-6-Output", "path": "/tmp/outputs/Output/data"}], "merge-avg-7":
      [{"key": "artifacts/$PIPELINERUN/merge-avg-7/Output.tgz", "name": "merge-avg-7-Output",
      "path": "/tmp/outputs/Output/data"}], "merge-avg-8": [{"key": "artifacts/$PIPELINERUN/merge-avg-8/Output.tgz",
      "name": "merge-avg-8-Output", "path": "/tmp/outputs/Output/data"}], "merge-avg-9":
      [{"key": "artifacts/$PIPELINERUN/merge-avg-9/Output.tgz", "name": "merge-avg-9-Output",
      "path": "/tmp/outputs/Output/data"}]}'
    tekton.dev/input_artifacts: '{"band-names": [{"name": "crop-tile-Output", "parent_task":
      "crop-tile"}, {"name": "pvc-eval-name", "parent_task": "pvc-eval"}], "band-names-2":
      [{"name": "crop-tile-2-Output", "parent_task": "crop-tile-2"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}], "band-names-3": [{"name": "crop-tile-3-Output",
      "parent_task": "crop-tile-3"}, {"name": "pvc-eval-name", "parent_task": "pvc-eval"}],
      "band-names-4": [{"name": "crop-tile-4-Output", "parent_task": "crop-tile-4"},
      {"name": "pvc-eval-name", "parent_task": "pvc-eval"}], "build-stack-eval": [{"name":
      "crop-region-eval-2-Output", "parent_task": "crop-region-eval-2"}, {"name":
      "crop-region-eval-3-Output", "parent_task": "crop-region-eval-3"}, {"name":
      "crop-region-eval-4-Output", "parent_task": "crop-region-eval-4"}, {"name":
      "crop-region-eval-Output", "parent_task": "crop-region-eval"}, {"name": "pvc-terrain-name",
      "parent_task": "pvc-terrain"}], "build-stack-train": [{"name": "merge-avg-Output",
      "parent_task": "merge-avg"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-10": [{"name": "merge-avg-10-Output",
      "parent_task": "merge-avg-10"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-11": [{"name": "merge-avg-11-Output",
      "parent_task": "merge-avg-11"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-12": [{"name": "merge-avg-12-Output",
      "parent_task": "merge-avg-12"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-2": [{"name": "merge-avg-2-Output",
      "parent_task": "merge-avg-2"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-3": [{"name": "merge-avg-3-Output",
      "parent_task": "merge-avg-3"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-4": [{"name": "merge-avg-4-Output",
      "parent_task": "merge-avg-4"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-5": [{"name": "merge-avg-5-Output",
      "parent_task": "merge-avg-5"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-6": [{"name": "merge-avg-6-Output",
      "parent_task": "merge-avg-6"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-7": [{"name": "merge-avg-7-Output",
      "parent_task": "merge-avg-7"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-8": [{"name": "merge-avg-8-Output",
      "parent_task": "merge-avg-8"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "build-stack-train-9": [{"name": "merge-avg-9-Output",
      "parent_task": "merge-avg-9"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "crop-region-eval": [{"name": "get-shp-Output",
      "parent_task": "get-shp"}, {"name": "pvc-shape-name", "parent_task": "pvc-shape"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}], "crop-region-eval-2":
      [{"name": "get-shp-Output", "parent_task": "get-shp"}, {"name": "pvc-shape-name",
      "parent_task": "pvc-shape"}, {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}],
      "crop-region-eval-3": [{"name": "get-shp-Output", "parent_task": "get-shp"},
      {"name": "pvc-shape-name", "parent_task": "pvc-shape"}, {"name": "pvc-terrain-name",
      "parent_task": "pvc-terrain"}], "crop-region-eval-4": [{"name": "get-shp-Output",
      "parent_task": "get-shp"}, {"name": "pvc-shape-name", "parent_task": "pvc-shape"},
      {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}], "crop-region-train":
      [{"name": "build-stack-train-Output", "parent_task": "build-stack-train"}, {"name":
      "get-shp-Output", "parent_task": "get-shp"}, {"name": "pvc-shape-name", "parent_task":
      "pvc-shape"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}], "crop-region-train-10":
      [{"name": "build-stack-train-10-Output", "parent_task": "build-stack-train-10"},
      {"name": "get-shp-Output", "parent_task": "get-shp"}, {"name": "pvc-shape-name",
      "parent_task": "pvc-shape"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}],
      "crop-region-train-11": [{"name": "build-stack-train-11-Output", "parent_task":
      "build-stack-train-11"}, {"name": "get-shp-Output", "parent_task": "get-shp"},
      {"name": "pvc-shape-name", "parent_task": "pvc-shape"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "crop-region-train-12": [{"name": "build-stack-train-12-Output",
      "parent_task": "build-stack-train-12"}, {"name": "get-shp-Output", "parent_task":
      "get-shp"}, {"name": "pvc-shape-name", "parent_task": "pvc-shape"}, {"name":
      "pvc-train-name", "parent_task": "pvc-train"}], "crop-region-train-2": [{"name":
      "build-stack-train-2-Output", "parent_task": "build-stack-train-2"}, {"name":
      "get-shp-Output", "parent_task": "get-shp"}, {"name": "pvc-shape-name", "parent_task":
      "pvc-shape"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}], "crop-region-train-3":
      [{"name": "build-stack-train-3-Output", "parent_task": "build-stack-train-3"},
      {"name": "get-shp-Output", "parent_task": "get-shp"}, {"name": "pvc-shape-name",
      "parent_task": "pvc-shape"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}],
      "crop-region-train-4": [{"name": "build-stack-train-4-Output", "parent_task":
      "build-stack-train-4"}, {"name": "get-shp-Output", "parent_task": "get-shp"},
      {"name": "pvc-shape-name", "parent_task": "pvc-shape"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "crop-region-train-5": [{"name": "build-stack-train-5-Output",
      "parent_task": "build-stack-train-5"}, {"name": "get-shp-Output", "parent_task":
      "get-shp"}, {"name": "pvc-shape-name", "parent_task": "pvc-shape"}, {"name":
      "pvc-train-name", "parent_task": "pvc-train"}], "crop-region-train-6": [{"name":
      "build-stack-train-6-Output", "parent_task": "build-stack-train-6"}, {"name":
      "get-shp-Output", "parent_task": "get-shp"}, {"name": "pvc-shape-name", "parent_task":
      "pvc-shape"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}], "crop-region-train-7":
      [{"name": "build-stack-train-7-Output", "parent_task": "build-stack-train-7"},
      {"name": "get-shp-Output", "parent_task": "get-shp"}, {"name": "pvc-shape-name",
      "parent_task": "pvc-shape"}, {"name": "pvc-train-name", "parent_task": "pvc-train"}],
      "crop-region-train-8": [{"name": "build-stack-train-8-Output", "parent_task":
      "build-stack-train-8"}, {"name": "get-shp-Output", "parent_task": "get-shp"},
      {"name": "pvc-shape-name", "parent_task": "pvc-shape"}, {"name": "pvc-train-name",
      "parent_task": "pvc-train"}], "crop-region-train-9": [{"name": "build-stack-train-9-Output",
      "parent_task": "build-stack-train-9"}, {"name": "get-shp-Output", "parent_task":
      "get-shp"}, {"name": "pvc-shape-name", "parent_task": "pvc-shape"}, {"name":
      "pvc-train-name", "parent_task": "pvc-train"}], "crop-tile": [{"name": "build-stack-eval-Output",
      "parent_task": "build-stack-eval"}, {"name": "pvc-eval-name", "parent_task":
      "pvc-eval"}, {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}], "crop-tile-2":
      [{"name": "build-stack-eval-Output", "parent_task": "build-stack-eval"}, {"name":
      "pvc-eval-name", "parent_task": "pvc-eval"}, {"name": "pvc-terrain-name", "parent_task":
      "pvc-terrain"}], "crop-tile-3": [{"name": "build-stack-eval-Output", "parent_task":
      "build-stack-eval"}, {"name": "pvc-eval-name", "parent_task": "pvc-eval"}, {"name":
      "pvc-terrain-name", "parent_task": "pvc-terrain"}], "crop-tile-4": [{"name":
      "build-stack-eval-Output", "parent_task": "build-stack-eval"}, {"name": "pvc-eval-name",
      "parent_task": "pvc-eval"}, {"name": "pvc-terrain-name", "parent_task": "pvc-terrain"}],
      "download-parallel": [{"name": "pvc-sm-name", "parent_task": "pvc-sm"}], "get-shp":
      [{"name": "pvc-shape-name", "parent_task": "pvc-shape"}], "merge-avg": [{"name":
      "download-parallel-Output", "parent_task": "download-parallel"}, {"name": "pvc-sm-name",
      "parent_task": "pvc-sm"}], "merge-avg-10": [{"name": "download-parallel-Output",
      "parent_task": "download-parallel"}, {"name": "pvc-sm-name", "parent_task":
      "pvc-sm"}], "merge-avg-11": [{"name": "download-parallel-Output", "parent_task":
      "download-parallel"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"}], "merge-avg-12":
      [{"name": "download-parallel-Output", "parent_task": "download-parallel"}, {"name":
      "pvc-sm-name", "parent_task": "pvc-sm"}], "merge-avg-2": [{"name": "download-parallel-Output",
      "parent_task": "download-parallel"}, {"name": "pvc-sm-name", "parent_task":
      "pvc-sm"}], "merge-avg-3": [{"name": "download-parallel-Output", "parent_task":
      "download-parallel"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"}], "merge-avg-4":
      [{"name": "download-parallel-Output", "parent_task": "download-parallel"}, {"name":
      "pvc-sm-name", "parent_task": "pvc-sm"}], "merge-avg-5": [{"name": "download-parallel-Output",
      "parent_task": "download-parallel"}, {"name": "pvc-sm-name", "parent_task":
      "pvc-sm"}], "merge-avg-6": [{"name": "download-parallel-Output", "parent_task":
      "download-parallel"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"}], "merge-avg-7":
      [{"name": "download-parallel-Output", "parent_task": "download-parallel"}, {"name":
      "pvc-sm-name", "parent_task": "pvc-sm"}], "merge-avg-8": [{"name": "download-parallel-Output",
      "parent_task": "download-parallel"}, {"name": "pvc-sm-name", "parent_task":
      "pvc-sm"}], "merge-avg-9": [{"name": "download-parallel-Output", "parent_task":
      "download-parallel"}, {"name": "pvc-sm-name", "parent_task": "pvc-sm"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"band-names": [], "band-names-2": [], "band-names-3":
      [], "band-names-4": [], "build-stack-eval": [["Output", "$(results.Output.path)"]],
      "build-stack-train": [["Output", "$(results.Output.path)"]], "build-stack-train-10":
      [["Output", "$(results.Output.path)"]], "build-stack-train-11": [["Output",
      "$(results.Output.path)"]], "build-stack-train-12": [["Output", "$(results.Output.path)"]],
      "build-stack-train-2": [["Output", "$(results.Output.path)"]], "build-stack-train-3":
      [["Output", "$(results.Output.path)"]], "build-stack-train-4": [["Output", "$(results.Output.path)"]],
      "build-stack-train-5": [["Output", "$(results.Output.path)"]], "build-stack-train-6":
      [["Output", "$(results.Output.path)"]], "build-stack-train-7": [["Output", "$(results.Output.path)"]],
      "build-stack-train-8": [["Output", "$(results.Output.path)"]], "build-stack-train-9":
      [["Output", "$(results.Output.path)"]], "crop-region-eval": [["Output", "$(results.Output.path)"]],
      "crop-region-eval-2": [["Output", "$(results.Output.path)"]], "crop-region-eval-3":
      [["Output", "$(results.Output.path)"]], "crop-region-eval-4": [["Output", "$(results.Output.path)"]],
      "crop-region-train": [], "crop-region-train-10": [], "crop-region-train-11":
      [], "crop-region-train-12": [], "crop-region-train-2": [], "crop-region-train-3":
      [], "crop-region-train-4": [], "crop-region-train-5": [], "crop-region-train-6":
      [], "crop-region-train-7": [], "crop-region-train-8": [], "crop-region-train-9":
      [], "crop-tile": [["Output", "$(results.Output.path)"]], "crop-tile-2": [["Output",
      "$(results.Output.path)"]], "crop-tile-3": [["Output", "$(results.Output.path)"]],
      "crop-tile-4": [["Output", "$(results.Output.path)"]], "download-parallel":
      [["Output", "$(results.Output.path)"]], "get-shp": [["Output", "$(results.Output.path)"]],
      "merge-avg": [["Output", "$(results.Output.path)"]], "merge-avg-10": [["Output",
      "$(results.Output.path)"]], "merge-avg-11": [["Output", "$(results.Output.path)"]],
      "merge-avg-12": [["Output", "$(results.Output.path)"]], "merge-avg-2": [["Output",
      "$(results.Output.path)"]], "merge-avg-3": [["Output", "$(results.Output.path)"]],
      "merge-avg-4": [["Output", "$(results.Output.path)"]], "merge-avg-5": [["Output",
      "$(results.Output.path)"]], "merge-avg-6": [["Output", "$(results.Output.path)"]],
      "merge-avg-7": [["Output", "$(results.Output.path)"]], "merge-avg-8": [["Output",
      "$(results.Output.path)"]], "merge-avg-9": [["Output", "$(results.Output.path)"]],
      "pvc-eval": [], "pvc-shape": [], "pvc-sm": [], "pvc-terrain": [], "pvc-train":
      []}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Pipeline for somospie
      data generation", "inputs": [{"default": "2010", "name": "year", "optional":
      true, "type": "Integer"}, {"default": "monthly", "name": "averaging_type", "optional":
      true, "type": "String"}, {"default": "icr.io/somospie/somospie-gdal-netcdf",
      "name": "container_image", "optional": true, "type": "String"}, {"default":
      "po-train", "name": "sm_name", "optional": true, "type": "String"}, {"default":
      "oklahoma-30m", "name": "terrain_name", "optional": true, "type": "String"},
      {"default": "oklahoma-27km", "name": "train_cos", "optional": true, "type":
      "String"}, {"default": "oklahoma-30m-eval", "name": "eval_cos", "optional":
      true, "type": "String"}, {"default": "po-shapes", "name": "shape_cos", "optional":
      true, "type": "String"}, {"default": "2", "name": "n_tiles", "optional": true,
      "type": "Integer"}, {"default": "EPSG:4326", "name": "projection", "optional":
      true, "type": "String"}, {"default": "oklahoma-30m", "name": "region_res", "optional":
      true, "type": "String"}], "name": "somospie data generation pipeline"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  params:
  - name: averaging_type
    value: monthly
  - name: container_image
    value: icr.io/somospie/somospie-gdal-netcdf
  - name: eval_cos
    value: oklahoma-30m-eval
  - name: n_tiles
    value: '2'
  - name: projection
    value: EPSG:4326
  - name: region_res
    value: oklahoma-30m
  - name: shape_cos
    value: po-shapes
  - name: sm_name
    value: po-train
  - name: terrain_name
    value: oklahoma-30m
  - name: train_cos
    value: oklahoma-27km
  - name: year
    value: '2010'
  pipelineSpec:
    params:
    - name: averaging_type
      default: monthly
    - name: container_image
      default: icr.io/somospie/somospie-gdal-netcdf
    - name: eval_cos
      default: oklahoma-30m-eval
    - name: n_tiles
      default: '2'
    - name: projection
      default: EPSG:4326
    - name: region_res
      default: oklahoma-30m
    - name: shape_cos
      default: po-shapes
    - name: sm_name
      default: po-train
    - name: terrain_name
      default: oklahoma-30m
    - name: train_cos
      default: oklahoma-27km
    - name: year
      default: '2010'
    tasks:
    - name: pvc-sm
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: sm_name
        value: $(params.sm_name)
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        - name: sm_name
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: $(inputs.params.sm_name)
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-sm
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
      retries: 3
    - name: pvc-terrain
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: terrain_name
        value: $(params.terrain_name)
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        - name: terrain_name
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: $(inputs.params.terrain_name)
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-terrain
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
      retries: 3
    - name: pvc-train
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: train_cos
        value: $(params.train_cos)
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        - name: train_cos
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: $(inputs.params.train_cos)
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-train
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
      retries: 3
    - name: pvc-eval
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: eval_cos
        value: $(params.eval_cos)
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        - name: eval_cos
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: $(inputs.params.eval_cos)
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-eval
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
      retries: 3
    - name: pvc-shape
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: shape_cos
        value: $(params.shape_cos)
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        - name: shape_cos
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: $(inputs.params.shape_cos)
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-shape
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
      retries: 3
    - name: get-shp
      params:
      - name: container_image
        value: $(params.container_image)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --zip-file
          - OK.zip
          - --dir
          - /shape/
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def get_shp(zip_file, dir):
                import zipfile
                import os
                from pathlib import Path

                Path(dir+'shp_file').mkdir(parents=True, exist_ok=True)
                with zipfile.ZipFile(dir+zip_file, 'r') as zip_ref:
                    zip_ref.extractall(dir+'shp_file')

                shp_files=[]
                for root, dirs, files in os.walk(dir+'shp_file'):
                    for file in files:
                        if file.endswith(".shp"):
                            shp_files.append(os.path.join(root, file))
                shp_file=shp_files[0]
                print(shp_file)
                return shp_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Get shp', description='')
            _parser.add_argument("--zip-file", dest="zip_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = get_shp(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: container_image
        - name: pvc-shape-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Get shp", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Get shp@sha256=cbe205049ec6014da6a52d67ee804c87ac0e428b23d41d95c61595eb49410e4a"}'
      retries: 3
    - name: download-parallel
      params:
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --year
          - $(inputs.params.year)
          - --dir
          - /sm/
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def download_parallel(year,dir):
                import calendar
                import subprocess
                import shutil
                from pathlib import Path
                import numpy as np
                import os
                import multiprocessing
                import multiprocess as mp

                def bash (argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq, stdout=subprocess.PIPE, stderr=subprocess.PIPE)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous
                    stdout, stderr = proc.communicate()

                    # Error catching: https://stackoverflow.com/questions/5826427/can-a-python-script-execute-a-function-inside-a-bash-script
                    if proc.returncode != 0:
                        raise RuntimeError("'%s' failed, error code: '%s', stdout: '%s', stderr: '%s'" % (
                            ' '.join(arg_seq), proc.returncode, stdout.rstrip(), stderr.rstrip()))

                def download(year, dir):
                    version = 7.1 # ESA CCI version
                    year_folder = './{0:04d}'.format(year)
                    Path(dir+year_folder).mkdir(parents=True, exist_ok=True)

                    commands = []
                    for month in range(1, 13):
                        for day in range(1, calendar.monthrange(year, month)[1] + 1):
                            download_link = 'ftp://anon-ftp.ceda.ac.uk/neodc/esacci/soil_moisture/data/daily_files/COMBINED/v0{0:.1f}/{1:04d}/ESACCI-SOILMOISTURE-L3S-SSMV-COMBINED-{1:04d}{2:02d}{3:02d}000000-fv0{0:.1f}.nc'.format(version, year, month, day)
                            #commands.append(['curl', '-C','-s', download_link, '-o', dir+'{0}/{1:02d}_{2:02d}.nc'.format(year_folder, month, day)])
                            commands.append(['wget', '-N','-c', download_link, '-O', dir+'{0}/{1:02d}_{2:02d}.nc'.format(year_folder, month, day)])
                            # bash(command)

                    pool=mp.Pool(multiprocessing.cpu_count())
                    pool.map(bash, commands)

                download(year,dir)
                return dir

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Download parallel', description='')
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = download_parallel(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: olayap/somospie-gdal-netcdf
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Download parallel",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Download
              parallel@sha256=01e425f3ad07bb77896a5c30414dd376e03d9ce7d55eb661a8ce48ceecfb0664"}'
      retries: 3
    - name: merge-avg
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '1'
          - --output-file
          - /sm/$(inputs.params.year)/01.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '2'
          - --output-file
          - /sm/$(inputs.params.year)/02.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '3'
          - --output-file
          - /sm/$(inputs.params.year)/03.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-4
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '4'
          - --output-file
          - /sm/$(inputs.params.year)/04.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-5
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '5'
          - --output-file
          - /sm/$(inputs.params.year)/05.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-6
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '6'
          - --output-file
          - /sm/$(inputs.params.year)/06.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-7
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '7'
          - --output-file
          - /sm/$(inputs.params.year)/07.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-8
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '8'
          - --output-file
          - /sm/$(inputs.params.year)/08.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-9
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '9'
          - --output-file
          - /sm/$(inputs.params.year)/09.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-10
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '10'
          - --output-file
          - /sm/$(inputs.params.year)/10.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-11
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '11'
          - --output-file
          - /sm/$(inputs.params.year)/11.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: merge-avg-12
      params:
      - name: container_image
        value: $(params.container_image)
      - name: download-parallel-Output
        value: $(tasks.download-parallel.results.Output)
      - name: projection
        value: $(params.projection)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: year
        value: $(params.year)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - $(inputs.params.download-parallel-Output)
          - --year
          - $(inputs.params.year)
          - --month
          - '12'
          - --output-file
          - /sm/$(inputs.params.year)/12.tif
          - --projection
          - $(inputs.params.projection)
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def merge_avg(dir, year, month, output_file, projection):
                from osgeo import gdal
                import glob
                import numpy as np
                import os
                import calendar
                import subprocess

                def bash(argv):
                    arg_seq = [str(arg) for arg in argv]
                    proc = subprocess.Popen(arg_seq)#, shell=True)
                    proc.wait() #... unless intentionally asynchronous

                def reproject(input_file, output_file, projection):
                    # Projection can be EPSG:4326, .... or the path to a wkt file
                    warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES', 'NUM_THREADS=ALL_CPUS'], multithread=True, warpOptions=['NUM_THREADS=ALL_CPUS'], dstNodata=np.nan, callback=gdal.TermProgress_nocb)
                    warp = gdal.Warp(output_file, input_file, options=warp_options)
                    warp = None  # Closes the files

                sm_files = ['NETCDF:'+dir+'{0:04d}/{1:02d}_{2:02d}.nc:sm'.format(year, month, day) for day in range(1, calendar.monthrange(year, month)[1])]

                vrt = gdal.BuildVRT(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), sm_files)
                vrt = None  # closes file

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'r') as f:
                    contents = f.read()

                if '<NoDataValue>' in contents:
                    nodata_value = contents[contents.index('<NoDataValue>') + len('<NoDataValue>'): contents.index('</NoDataValue>')]# To add averaging function
                else:
                    nodata_value = 0

                code = '''band="1" subClass="VRTDerivedRasterBand">
              <PixelFunctionType>average</PixelFunctionType>
              <PixelFunctionLanguage>Python</PixelFunctionLanguage>
              <PixelFunctionCode><![CDATA[
            import numpy as np

            def average(in_ar, out_ar, xoff, yoff, xsize, ysize, raster_xsize,raster_ysize, buf_radius, gt, **kwargs):
                data = np.ma.array(in_ar, mask=np.equal(in_ar, {}))
                np.ma.mean(data, axis=0, out=out_ar, dtype="float32")
                mask = np.all(data.mask,axis = 0)
                out_ar[mask] = {}
            ]]>
              </PixelFunctionCode>'''.format(nodata_value, nodata_value)

                sub1, sub2 = contents.split('band="1">', 1)
                contents = sub1 + code + sub2

                with open(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), 'w') as f:
                    f.write(contents)

                cmd = ['gdal_translate', '-co', 'COMPRESS=LZW', '-co', 'TILED=YES', '-co', 'BIGTIFF=YES', '--config', 'GDAL_VRT_ENABLE_PYTHON', 'YES', dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month), output_file]
                bash(cmd)
                os.remove(dir+'{0:04d}_{1:02d}_merged.vrt'.format(year, month))

                reproject(output_file, output_file, projection)

                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Merge avg', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--projection", dest="projection", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = merge_avg(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
        params:
        - name: container_image
        - name: download-parallel-Output
        - name: projection
        - name: pvc-sm-name
        - name: year
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Merge avg", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Merge avg@sha256=fd92360d8f85c455819c64774b901c18d2b494f5484d0f30411bfaf6e3d5968d"}'
      retries: 3
    - name: build-stack-train
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-Output
        value: $(tasks.merge-avg.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_01.tif
          - --year
          - '2010'
          - --month
          - '1'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train
      params:
      - name: build-stack-train-Output
        value: $(tasks.build-stack-train.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_01.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '1'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-2-Output
        value: $(tasks.merge-avg-2.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-2-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_02.tif
          - --year
          - '2010'
          - --month
          - '2'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-2-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-2
      params:
      - name: build-stack-train-2-Output
        value: $(tasks.build-stack-train-2.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-2-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_02.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '2'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-2-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-3-Output
        value: $(tasks.merge-avg-3.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-3-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_03.tif
          - --year
          - '2010'
          - --month
          - '3'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-3-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-3
      params:
      - name: build-stack-train-3-Output
        value: $(tasks.build-stack-train-3.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-3-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_03.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '3'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-3-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-4
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-4-Output
        value: $(tasks.merge-avg-4.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-4-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_04.tif
          - --year
          - '2010'
          - --month
          - '4'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-4-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-4
      params:
      - name: build-stack-train-4-Output
        value: $(tasks.build-stack-train-4.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-4-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_04.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '4'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-4-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-5
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-5-Output
        value: $(tasks.merge-avg-5.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-5-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_05.tif
          - --year
          - '2010'
          - --month
          - '5'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-5-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-5
      params:
      - name: build-stack-train-5-Output
        value: $(tasks.build-stack-train-5.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-5-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_05.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '5'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-5-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-6
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-6-Output
        value: $(tasks.merge-avg-6.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-6-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_06.tif
          - --year
          - '2010'
          - --month
          - '6'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-6-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-6
      params:
      - name: build-stack-train-6-Output
        value: $(tasks.build-stack-train-6.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-6-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_06.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '6'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-6-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-7
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-7-Output
        value: $(tasks.merge-avg-7.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-7-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_07.tif
          - --year
          - '2010'
          - --month
          - '7'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-7-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-7
      params:
      - name: build-stack-train-7-Output
        value: $(tasks.build-stack-train-7.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-7-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_07.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '7'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-7-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-8
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-8-Output
        value: $(tasks.merge-avg-8.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-8-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_08.tif
          - --year
          - '2010'
          - --month
          - '8'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-8-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-8
      params:
      - name: build-stack-train-8-Output
        value: $(tasks.build-stack-train-8.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-8-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_08.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '8'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-8-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-9
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-9-Output
        value: $(tasks.merge-avg-9.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-9-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_09.tif
          - --year
          - '2010'
          - --month
          - '9'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-9-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-9
      params:
      - name: build-stack-train-9-Output
        value: $(tasks.build-stack-train-9.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-9-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_09.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '9'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-9-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-10
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-10-Output
        value: $(tasks.merge-avg-10.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-10-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_10.tif
          - --year
          - '2010'
          - --month
          - '10'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-10-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-10
      params:
      - name: build-stack-train-10-Output
        value: $(tasks.build-stack-train-10.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-10-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_10.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '10'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-10-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-11
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-11-Output
        value: $(tasks.merge-avg-11.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-11-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_11.tif
          - --year
          - '2010'
          - --month
          - '11'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-11-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-11
      params:
      - name: build-stack-train-11-Output
        value: $(tasks.build-stack-train-11.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-11-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_11.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '11'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-11-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: build-stack-train-12
      params:
      - name: container_image
        value: $(params.container_image)
      - name: merge-avg-12-Output
        value: $(tasks.merge-avg-12.results.Output)
      - name: pvc-sm-name
        value: $(tasks.pvc-sm.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --dir
          - /train/
          - --satellite-file
          - $(inputs.params.merge-avg-12-Output)
          - --input-files
          - '["/terrain/aspect_reprojected.tif", "/terrain/elevation_reprojected.tif",
            "/terrain/hillshading_reprojected.tif", "/terrain/slope_reprojected.tif"]'
          - --output-file
          - /train/2010_12.tif
          - --year
          - '2010'
          - --month
          - '12'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_train(dir,satellite_file, input_files, output_file, year, month):

                from osgeo import gdal
                import os

                # input_files: list of .tif files to stack
                input_files.insert(0, satellite_file)

                # Get target resolution from satellite file
                ds = gdal.Open(input_files[0], 0)
                xmin, xres, _, ymax, _, yres = ds.GetGeoTransform()
                for i in input_files:
                    rds=gdal.Open(i)

                vrt_file = dir+'{0:04d}_{1:02d}_stack.vrt'.format(year, month)
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], xRes=xres , yRes=yres,
                                                          callback=gdal.TermProgress_nocb)
                gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                os.remove(vrt_file)
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack train', description='')
            _parser.add_argument("--dir", dest="dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--satellite-file", dest="satellite_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_train(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /sm/
            name: pvc-sm
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /train/
            name: pvc-train
        params:
        - name: container_image
        - name: merge-avg-12-Output
        - name: pvc-sm-name
        - name: pvc-terrain-name
        - name: pvc-train-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-sm
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-sm-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack train",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack train@sha256=a5d4f88ee8abb338168aa10845d39dae8e81003fec549742741ee45b5bfce5e8"}'
      retries: 3
    - name: crop-region-train-12
      params:
      - name: build-stack-train-12-Output
        value: $(tasks.build-stack-train-12.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-train-name
        value: $(tasks.pvc-train.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - $(inputs.params.build-stack-train-12-Output)
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /train/2010_12.tif
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          - --year
          - '2010'
          - --month
          - '12'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_train(input_file, shp_file, output_file, parameter_names, year, month):
                from osgeo import gdal
                from pathlib import Path

                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    ds = None

                parameter_names.insert(0, 'z')
                print(parameter_names)

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                                                callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None

                set_band_names(output_file, parameter_names)
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region train', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--year", dest="year", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--month", dest="month", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = crop_region_train(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /train/
            name: pvc-train
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: build-stack-train-12-Output
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-train-name
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-train
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-train-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region train",
              "outputs": [], "version": "Crop region train@sha256=d8e9c4e2da42d9abe14eb19ef7678b069dd4039d72e1b10e388bea4cf25c07df"}'
      retries: 3
    - name: crop-region-eval
      params:
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - /terrain/aspect_reprojected.tif
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /terrain/aspect_cropped.tif
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_eval(input_file, shp_file, output_file):
                from osgeo import gdal

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region eval', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = crop_region_eval(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region eval",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Crop
              region eval@sha256=60e08bc4d15fba04127ade5346ab4716f0bfab559b0815b5d471e859808e4621"}'
      retries: 3
    - name: crop-region-eval-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - /terrain/elevation_reprojected.tif
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /terrain/elevation_cropped.tif
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_eval(input_file, shp_file, output_file):
                from osgeo import gdal

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region eval', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = crop_region_eval(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region eval",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Crop
              region eval@sha256=60e08bc4d15fba04127ade5346ab4716f0bfab559b0815b5d471e859808e4621"}'
      retries: 3
    - name: crop-region-eval-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - /terrain/hillshading_reprojected.tif
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /terrain/hillshading_cropped.tif
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_eval(input_file, shp_file, output_file):
                from osgeo import gdal

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region eval', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = crop_region_eval(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region eval",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Crop
              region eval@sha256=60e08bc4d15fba04127ade5346ab4716f0bfab559b0815b5d471e859808e4621"}'
      retries: 3
    - name: crop-region-eval-4
      params:
      - name: container_image
        value: $(params.container_image)
      - name: get-shp-Output
        value: $(tasks.get-shp.results.Output)
      - name: pvc-shape-name
        value: $(tasks.pvc-shape.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-file
          - /terrain/slope_reprojected.tif
          - --shp-file
          - $(inputs.params.get-shp-Output)
          - --output-file
          - /terrain/slope_cropped.tif
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_region_eval(input_file, shp_file, output_file):
                from osgeo import gdal

                warp_options = gdal.WarpOptions(cutlineDSName=shp_file, cropToCutline=True, creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)
                warp = gdal.Warp(output_file, input_file, options=warp_options)
                warp = None
                return output_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Crop region eval', description='')
            _parser.add_argument("--input-file", dest="input_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--shp-file", dest="shp_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = crop_region_eval(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          volumeMounts:
          - mountPath: /terrain/
            name: pvc-terrain
          - mountPath: /shape/
            name: pvc-shape
        params:
        - name: container_image
        - name: get-shp-Output
        - name: pvc-shape-name
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-shape
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-shape-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop region eval",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Crop
              region eval@sha256=60e08bc4d15fba04127ade5346ab4716f0bfab559b0815b5d471e859808e4621"}'
      retries: 3
    - name: build-stack-eval
      params:
      - name: container_image
        value: $(params.container_image)
      - name: crop-region-eval-2-Output
        value: $(tasks.crop-region-eval-2.results.Output)
      - name: crop-region-eval-3-Output
        value: $(tasks.crop-region-eval-3.results.Output)
      - name: crop-region-eval-4-Output
        value: $(tasks.crop-region-eval-4.results.Output)
      - name: crop-region-eval-Output
        value: $(tasks.crop-region-eval.results.Output)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-files
          - '["$(inputs.params.crop-region-eval-Output)", "$(inputs.params.crop-region-eval-2-Output)",
            "$(inputs.params.crop-region-eval-3-Output)", "$(inputs.params.crop-region-eval-4-Output)"]'
          - --vrt-file
          - /terrain/stack.vrt
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def build_stack_eval(input_files, vrt_file):

                from osgeo import gdal, ogr  # Install in a conda env: https://anaconda.org/conda-forge/gdal

                # input_files: list of .tif files to stack
                vrt_options = gdal.BuildVRTOptions(separate=True)
                vrt = gdal.BuildVRT(vrt_file, input_files, options=vrt_options)
                #translate_options = gdal.TranslateOptions(creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'],
                #                                          callback=gdal.TermProgress_nocb)
                #gdal.Translate(output_file, vrt, options=translate_options)
                vrt = None  # closes file
                return vrt_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Build stack eval', description='')
            _parser.add_argument("--input-files", dest="input_files", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--vrt-file", dest="vrt_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = build_stack_eval(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /terrain/
            name: pvc-terrain
        params:
        - name: container_image
        - name: crop-region-eval-2-Output
        - name: crop-region-eval-3-Output
        - name: crop-region-eval-4-Output
        - name: crop-region-eval-Output
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Build stack eval",
              "outputs": [{"name": "Output", "type": "String"}], "version": "Build
              stack eval@sha256=9f752cac5cb44da636fa4ab0708a2c099e999dfbc5a55343ffa045d0c58e45a8"}'
      retries: 3
    - name: crop-tile
      params:
      - name: build-stack-eval-Output
        value: $(tasks.build-stack-eval.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --raster
          - $(inputs.params.build-stack-eval-Output)
          - --out-file
          - /eval/ok-30m_eval_0000.tif
          - --n-tiles
          - '2'
          - --idx-x
          - '0'
          - --idx-y
          - '0'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_tile(raster, out_file, n_tiles, idx_x, idx_y):
                from osgeo import gdal
                import numpy as np
                import math

                # idx_x number of the tile in the x dimension
                ds = gdal.Open(raster, 0)
                cols = ds.RasterXSize
                rows = ds.RasterYSize
                x_win_size = int(math.ceil(cols / n_tiles))
                y_win_size = int(math.ceil(rows / n_tiles))

                idx_x = range(0, cols, x_win_size)[idx_x]
                idx_y = range(0, rows, y_win_size)[idx_y]

                if idx_y + y_win_size < rows:
                    nrows = y_win_size
                else:
                    nrows = rows - idx_y

                if idx_x + x_win_size < cols:
                    ncols = x_win_size
                else:
                    ncols = cols - idx_x

                translate_options = gdal.TranslateOptions(srcWin=[idx_x, idx_y, ncols, nrows], creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)
                gdal.Translate(out_file, raster, options=translate_options)
                return out_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Crop tile', description='')
            _parser.add_argument("--raster", dest="raster", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--out-file", dest="out_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--n-tiles", dest="n_tiles", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--idx-x", dest="idx_x", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--idx-y", dest="idx_y", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = crop_tile(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /terrain/
            name: pvc-terrain
        params:
        - name: build-stack-eval-Output
        - name: container_image
        - name: pvc-eval-name
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop tile", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Crop tile@sha256=6425ba1201a4dfe5f6a5d4fc8f8f386345c9e6113562606fe9ab5c8d25609e70"}'
      retries: 3
    - name: band-names
      params:
      - name: container_image
        value: $(params.container_image)
      - name: crop-tile-Output
        value: $(tasks.crop-tile.results.Output)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --output-file
          - $(inputs.params.crop-tile-Output)
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def band_names(output_file, parameter_names):#, vrt_file:str):
                from osgeo import gdal
                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    print(ds.RasterCount)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    del ds

                set_band_names(output_file, parameter_names)
                #os.remove(vrt_file)
                print("Band names:")
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Band names', description='')
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = band_names(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /eval/
            name: pvc-eval
        params:
        - name: container_image
        - name: crop-tile-Output
        - name: pvc-eval-name
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Band names",
              "outputs": [], "version": "Band names@sha256=43a98355b9e36e490c92c07a1ce4572fb71d87a29de23a5db96132086e8ce898"}'
      retries: 3
    - name: crop-tile-2
      params:
      - name: build-stack-eval-Output
        value: $(tasks.build-stack-eval.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --raster
          - $(inputs.params.build-stack-eval-Output)
          - --out-file
          - /eval/ok-30m_eval_0001.tif
          - --n-tiles
          - '2'
          - --idx-x
          - '0'
          - --idx-y
          - '1'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_tile(raster, out_file, n_tiles, idx_x, idx_y):
                from osgeo import gdal
                import numpy as np
                import math

                # idx_x number of the tile in the x dimension
                ds = gdal.Open(raster, 0)
                cols = ds.RasterXSize
                rows = ds.RasterYSize
                x_win_size = int(math.ceil(cols / n_tiles))
                y_win_size = int(math.ceil(rows / n_tiles))

                idx_x = range(0, cols, x_win_size)[idx_x]
                idx_y = range(0, rows, y_win_size)[idx_y]

                if idx_y + y_win_size < rows:
                    nrows = y_win_size
                else:
                    nrows = rows - idx_y

                if idx_x + x_win_size < cols:
                    ncols = x_win_size
                else:
                    ncols = cols - idx_x

                translate_options = gdal.TranslateOptions(srcWin=[idx_x, idx_y, ncols, nrows], creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)
                gdal.Translate(out_file, raster, options=translate_options)
                return out_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Crop tile', description='')
            _parser.add_argument("--raster", dest="raster", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--out-file", dest="out_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--n-tiles", dest="n_tiles", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--idx-x", dest="idx_x", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--idx-y", dest="idx_y", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = crop_tile(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /terrain/
            name: pvc-terrain
        params:
        - name: build-stack-eval-Output
        - name: container_image
        - name: pvc-eval-name
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop tile", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Crop tile@sha256=6425ba1201a4dfe5f6a5d4fc8f8f386345c9e6113562606fe9ab5c8d25609e70"}'
      retries: 3
    - name: band-names-2
      params:
      - name: container_image
        value: $(params.container_image)
      - name: crop-tile-2-Output
        value: $(tasks.crop-tile-2.results.Output)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --output-file
          - $(inputs.params.crop-tile-2-Output)
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def band_names(output_file, parameter_names):#, vrt_file:str):
                from osgeo import gdal
                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    print(ds.RasterCount)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    del ds

                set_band_names(output_file, parameter_names)
                #os.remove(vrt_file)
                print("Band names:")
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Band names', description='')
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = band_names(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /eval/
            name: pvc-eval
        params:
        - name: container_image
        - name: crop-tile-2-Output
        - name: pvc-eval-name
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Band names",
              "outputs": [], "version": "Band names@sha256=43a98355b9e36e490c92c07a1ce4572fb71d87a29de23a5db96132086e8ce898"}'
      retries: 3
    - name: crop-tile-3
      params:
      - name: build-stack-eval-Output
        value: $(tasks.build-stack-eval.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --raster
          - $(inputs.params.build-stack-eval-Output)
          - --out-file
          - /eval/ok-30m_eval_0002.tif
          - --n-tiles
          - '2'
          - --idx-x
          - '1'
          - --idx-y
          - '0'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_tile(raster, out_file, n_tiles, idx_x, idx_y):
                from osgeo import gdal
                import numpy as np
                import math

                # idx_x number of the tile in the x dimension
                ds = gdal.Open(raster, 0)
                cols = ds.RasterXSize
                rows = ds.RasterYSize
                x_win_size = int(math.ceil(cols / n_tiles))
                y_win_size = int(math.ceil(rows / n_tiles))

                idx_x = range(0, cols, x_win_size)[idx_x]
                idx_y = range(0, rows, y_win_size)[idx_y]

                if idx_y + y_win_size < rows:
                    nrows = y_win_size
                else:
                    nrows = rows - idx_y

                if idx_x + x_win_size < cols:
                    ncols = x_win_size
                else:
                    ncols = cols - idx_x

                translate_options = gdal.TranslateOptions(srcWin=[idx_x, idx_y, ncols, nrows], creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)
                gdal.Translate(out_file, raster, options=translate_options)
                return out_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Crop tile', description='')
            _parser.add_argument("--raster", dest="raster", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--out-file", dest="out_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--n-tiles", dest="n_tiles", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--idx-x", dest="idx_x", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--idx-y", dest="idx_y", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = crop_tile(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /terrain/
            name: pvc-terrain
        params:
        - name: build-stack-eval-Output
        - name: container_image
        - name: pvc-eval-name
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop tile", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Crop tile@sha256=6425ba1201a4dfe5f6a5d4fc8f8f386345c9e6113562606fe9ab5c8d25609e70"}'
      retries: 3
    - name: band-names-3
      params:
      - name: container_image
        value: $(params.container_image)
      - name: crop-tile-3-Output
        value: $(tasks.crop-tile-3.results.Output)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --output-file
          - $(inputs.params.crop-tile-3-Output)
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def band_names(output_file, parameter_names):#, vrt_file:str):
                from osgeo import gdal
                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    print(ds.RasterCount)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    del ds

                set_band_names(output_file, parameter_names)
                #os.remove(vrt_file)
                print("Band names:")
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Band names', description='')
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = band_names(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /eval/
            name: pvc-eval
        params:
        - name: container_image
        - name: crop-tile-3-Output
        - name: pvc-eval-name
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Band names",
              "outputs": [], "version": "Band names@sha256=43a98355b9e36e490c92c07a1ce4572fb71d87a29de23a5db96132086e8ce898"}'
      retries: 3
    - name: crop-tile-4
      params:
      - name: build-stack-eval-Output
        value: $(tasks.build-stack-eval.results.Output)
      - name: container_image
        value: $(params.container_image)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      - name: pvc-terrain-name
        value: $(tasks.pvc-terrain.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --raster
          - $(inputs.params.build-stack-eval-Output)
          - --out-file
          - /eval/ok-30m_eval_0003.tif
          - --n-tiles
          - '2'
          - --idx-x
          - '1'
          - --idx-y
          - '1'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def crop_tile(raster, out_file, n_tiles, idx_x, idx_y):
                from osgeo import gdal
                import numpy as np
                import math

                # idx_x number of the tile in the x dimension
                ds = gdal.Open(raster, 0)
                cols = ds.RasterXSize
                rows = ds.RasterYSize
                x_win_size = int(math.ceil(cols / n_tiles))
                y_win_size = int(math.ceil(rows / n_tiles))

                idx_x = range(0, cols, x_win_size)[idx_x]
                idx_y = range(0, rows, y_win_size)[idx_y]

                if idx_y + y_win_size < rows:
                    nrows = y_win_size
                else:
                    nrows = rows - idx_y

                if idx_x + x_win_size < cols:
                    ncols = x_win_size
                else:
                    ncols = cols - idx_x

                translate_options = gdal.TranslateOptions(srcWin=[idx_x, idx_y, ncols, nrows], creationOptions=['COMPRESS=LZW', 'TILED=YES', 'BIGTIFF=YES'], callback=gdal.TermProgress_nocb)
                gdal.Translate(out_file, raster, options=translate_options)
                return out_file

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Crop tile', description='')
            _parser.add_argument("--raster", dest="raster", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--out-file", dest="out_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--n-tiles", dest="n_tiles", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--idx-x", dest="idx_x", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--idx-y", dest="idx_y", type=int, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = crop_tile(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /eval/
            name: pvc-eval
          - mountPath: /terrain/
            name: pvc-terrain
        params:
        - name: build-stack-eval-Output
        - name: container_image
        - name: pvc-eval-name
        - name: pvc-terrain-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        - name: pvc-terrain
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-terrain-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Crop tile", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Crop tile@sha256=6425ba1201a4dfe5f6a5d4fc8f8f386345c9e6113562606fe9ab5c8d25609e70"}'
      retries: 3
    - name: band-names-4
      params:
      - name: container_image
        value: $(params.container_image)
      - name: crop-tile-4-Output
        value: $(tasks.crop-tile-4.results.Output)
      - name: pvc-eval-name
        value: $(tasks.pvc-eval.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --output-file
          - $(inputs.params.crop-tile-4-Output)
          - --parameter-names
          - '["aspect", "elevation", "hillshading", "slope"]'
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def band_names(output_file, parameter_names):#, vrt_file:str):
                from osgeo import gdal
                def get_band_names(raster):
                    ds = gdal.Open(raster, 0)
                    names = []
                    for band in range(ds.RasterCount):
                        b = ds.GetRasterBand(band + 1)
                        names.append(b.GetDescription())
                    ds = None
                    return names

                def set_band_names(raster, band_names):
                    ds = gdal.Open(raster, 0)
                    print(ds.RasterCount)
                    for i, name in enumerate(band_names):
                        b = ds.GetRasterBand(i + 1)
                        b.SetDescription(name)
                    del ds

                set_band_names(output_file, parameter_names)
                #os.remove(vrt_file)
                print("Band names:")
                print(get_band_names(output_file))

            import json
            import argparse
            _parser = argparse.ArgumentParser(prog='Band names', description='')
            _parser.add_argument("--output-file", dest="output_file", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--parameter-names", dest="parameter_names", type=json.loads, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = band_names(**_parsed_args)
          image: $(inputs.params.container_image)
          resources:
            requests:
              memory: 1G
          volumeMounts:
          - mountPath: /eval/
            name: pvc-eval
        params:
        - name: container_image
        - name: crop-tile-4-Output
        - name: pvc-eval-name
        volumes:
        - name: pvc-eval
          persistentVolumeClaim:
            claimName: $(inputs.params.pvc-eval-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Band names",
              "outputs": [], "version": "Band names@sha256=43a98355b9e36e490c92c07a1ce4572fb71d87a29de23a5db96132086e8ce898"}'
      retries: 3
