apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: somospiepipeline
  annotations:
    tekton.dev/output_artifacts: '{"knn-train": [{"key": "artifacts/$PIPELINERUN/knn-train/Output.tgz",
      "name": "knn-train-Output", "path": "/tmp/outputs/Output/data"}], "load-data":
      [{"key": "artifacts/$PIPELINERUN/load-data/Output.tgz", "name": "load-data-Output",
      "path": "/tmp/outputs/Output/data"}]}'
    tekton.dev/input_artifacts: '{"knn-inference": [{"name": "knn-train-Output", "parent_task":
      "knn-train"}, {"name": "odh-pvc-name", "parent_task": "odh-pvc"}], "knn-train":
      [{"name": "load-data-Output", "parent_task": "load-data"}, {"name": "odh-pvc-name",
      "parent_task": "odh-pvc"}], "load-data": [{"name": "odh-pvc-name", "parent_task":
      "odh-pvc"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"knn-inference": [], "knn-train": [["Output", "$(results.Output.path)"]],
      "load-data": [["Output", "$(results.Output.path)"]], "odh-pvc": []}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Pipeline for somospie",
      "name": "somospiepipeline"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  pipelineSpec:
    tasks:
    - name: odh-pvc
      params:
      - name: action
        value: create
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      taskSpec:
        params:
        - description: Action on the resource
          name: action
          type: string
        - default: strategic
          description: Merge strategy when using action patch
          name: merge-strategy
          type: string
        - default: ''
          description: An express to retrieval data from resource.
          name: output
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is success.
          name: success-condition
          type: string
        - default: ''
          description: A label selector express to decide if the action on resource
            is failure.
          name: failure-condition
          type: string
        - default: quay.io/aipipeline/kubectl-wrapper:latest
          description: Kubectl wrapper image
          name: image
          type: string
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        steps:
        - command:
          - kubeclient
          args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              annotations:
                ibm.io/auto-create-bucket: 'false'
                ibm.io/auto-delete-bucket: 'false'
                ibm.io/bucket: odh-oklahoma1km
                ibm.io/endpoint: https://s3.us-east.cloud-object-storage.appdomain.cloud
                ibm.io/secret-name: po-secret
              name: $(PIPELINERUN)-pvc-odh-oklahoma1km
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 10Gi
              storageClassName: ibmc-s3fs-standard-regional
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
        results:
        - name: manifest
          type: string
          description: '{}'
        - name: name
          type: string
          description: '{.metadata.name}'
        - name: size
          type: string
          description: '{.status.capacity.storage}'
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
    - name: load-data
      params:
      - name: odh-pvc-name
        value: $(tasks.odh-pvc.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --input-path
          - /cos/
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def load_data(input_path):
                import numpy as np
                import json
                import argparse
                import pandas as pd
                from pathlib import Path
                import pickle
                from sklearn.preprocessing import StandardScaler
                from sklearn.model_selection import train_test_split
                import os

                print("Reading training data from", input_path)
                print("Current directory:", Path.cwd(), " and full path ", Path(__file__).resolve().parent)
                print("Files and directory in the bucket", os.listdir(Path.cwd()))
                training_data = pd.read_csv(input_path+"train.csv")
                col = list(training_data.columns)
                col[2] = 'z'
                training_data.columns = col

                x_train, x_test, y_train, y_test = train_test_split(training_data.loc[:,training_data.columns != 'z'], training_data.loc[:,'z'], test_size=0.1)
                #print(x_train,"\n",y_train,"TEST\n",x_test,y_test)
                ss = StandardScaler()
                x_train = ss.fit_transform(x_train)
                x_test = ss.transform(x_test)
                #print("SCALED")
                #print(x_train,"\n",y_train,"TEST\n",x_test,y_test)

                # Save scaler model so it can be reused for predicting
                pickle.dump(ss, open(input_path+'scaler.pkl', 'wb'))

                # Save data to train with different ml-models
                data = {'x_train' : x_train.tolist(),
                        'y_train' : y_train.tolist(),
                        'x_test' : x_test.tolist(),
                        'y_test' : y_test.tolist()}
                # Creates a json object based on `data`
                data_json = json.dumps(data)

                # Saves the json object into a file
                with open(input_path+"data.json", 'w') as out_file:
                    json.dump(data_json, out_file)

                   # Create path to where data and scaler are saved

                return input_path

            def _serialize_str(str_value: str) -> str:
                if not isinstance(str_value, str):
                    raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                        str(str_value), str(type(str_value))))
                return str_value

            import argparse
            _parser = argparse.ArgumentParser(prog='Load data', description='')
            _parser.add_argument("--input-path", dest="input_path", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = load_data(**_parsed_args)

            _outputs = [_outputs]

            _output_serializers = [
                _serialize_str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: olayap/somospie
          volumeMounts:
          - mountPath: /cos/
            name: odh-pvc
        params:
        - name: odh-pvc-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: odh-pvc
          persistentVolumeClaim:
            claimName: $(inputs.params.odh-pvc-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Load data", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Load data@sha256=45c85c25cbf3c2702752cc9c47af825f936d4ca2c35acd1da82e972ebec48ccd"}'
    - name: knn-train
      params:
      - name: load-data-Output
        value: $(tasks.load-data.results.Output)
      - name: odh-pvc-name
        value: $(tasks.odh-pvc.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --data-path
          - $(inputs.params.load-data-Output)
          - --k
          - '20'
          - --seed
          - '3'
          - '----output-paths'
          - $(results.Output.path)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_train (data_path, k, seed):\n    import numpy as np\n    import\
            \ pandas as pd\n    import argparse\n    import pickle\n    import json\n\
            \    from pathlib import Path\n    from sklearn.neighbors import KNeighborsRegressor\n\
            \    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing\
            \ import StandardScaler\n    from sklearn.model_selection import RandomizedSearchCV\n\
            \    from sklearn.metrics import mean_squared_error\n\n    # Define functions\
            \ for training\n    def random_parameter_search(knn, x_train, y_train,\
            \ maxK, seed):\n        # Dictionary with all the hyperparameter options\
            \ for the knn model: n_neighbors, weights, metric\n        params = {'n_neighbors':\
            \ list(range(2,maxK)),\n            'weights': ['uniform','distance',\
            \ gaussian],\n            'metric': ['euclidean','minkowski']\n      \
            \          }\n        # Random search based on the grid of params and\
            \ n_iter controls number of random combinations it will try\n        #\
            \ n_jobs=-1 means using all processors\n        # random_state sets the\
            \ seed for manner of reproducibility \n        params_search = RandomizedSearchCV(knn,\
            \ params, verbose=1, cv=10, n_iter=50, random_state=seed, n_jobs=-1)\n\
            \        params_search.fit(x_train,y_train)\n        # Check the results\
            \ from the parameter search  \n        print(params_search.best_score_)\n\
            \        print(params_search.best_params_)\n        print(params_search.best_estimator_)\n\
            \        return params_search.best_params_\n\n    def gaussian(dist, sigma\
            \ = 4):\n        # Input a distance and return its weight using the gaussian\
            \ kernel \n        weight = np.exp(-dist**2/(2*sigma**2))\n        return\
            \ weight\n\n    def validate_knn(knn, x_test, y_test):\n        # Predict\
            \ on x_test\n        y_test_predicted = knn.predict(x_test)\n        #\
            \ Measure the rmse\n        rmse = np.sqrt(mean_squared_error(y_test,\
            \ y_test_predicted))\n        # Print error\t\n        #print(\"Predictions\
            \ of soil moisture:\", y_test_predicted)\n        #print(\"Original values\
            \ of soil moisture:\", y_test)\n        print(\"The rmse for the validation\
            \ is:\", rmse)\n\n    print(\"Reading training data from\", data_path)\n\
            \        # Open and reads file \"data\"\n    with open(data_path+\"data.json\"\
            ) as data_file:\n        data = json.load(data_file)\n\n    data = json.loads(data)\n\
            \n    x_train = data['x_train']\n    y_train = data['y_train']\n    x_test\
            \ = data['x_test']\n    y_test = data['y_test']\n    #print(training_data)\n\
            \    maxK = int(k)\n    seed = int(seed)\n\n        # Define initial model\n\
            \    knn = KNeighborsRegressor()\n    # Random parameter search of n_neighbors,\
            \ weigths and metric\n    best_params = random_parameter_search(knn, x_train,\
            \ y_train, maxK, seed)\n    # Based on selection build the new regressor\n\
            \    knn = KNeighborsRegressor(n_neighbors=best_params['n_neighbors'],\
            \ weights=best_params['weights'],\n    \t\t\t\tmetric=best_params['metric'],\
            \ n_jobs=-1)\n    # Fit the new model to data\n    knn.fit(x_train, y_train)\n\
            \    # Save model\n\n    #Path(args.pathtomodel).parent.mkdir(parents=True,\
            \ exist_ok=True)\n    pickle.dump(knn, open(data_path+'model.pkl', 'wb'))\n\
            \n    # Validate\n    validate_knn(knn, x_test, y_test)\n    return data_path\n\
            \ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
            \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of str.'.format(\n            str(str_value), str(type(str_value))))\n\
            \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn\
            \ train', description='')\n_parser.add_argument(\"--data-path\", dest=\"\
            data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --k\", dest=\"k\", type=int, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--seed\", dest=\"seed\", type=int, required=True,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\"\
            , dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ knn_train(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: olayap/somospie
          volumeMounts:
          - mountPath: /cos/
            name: odh-pvc
        params:
        - name: load-data-Output
        - name: odh-pvc-name
        results:
        - name: Output
          type: string
          description: /tmp/outputs/Output/data
        volumes:
        - name: odh-pvc
          persistentVolumeClaim:
            claimName: $(inputs.params.odh-pvc-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn train", "outputs":
              [{"name": "Output", "type": "String"}], "version": "Knn train@sha256=d0734bc9c76d89e28bc4ba801b37b46400e267e619c5f434d3121e9c69fb190a"}'
    - name: knn-inference
      params:
      - name: knn-train-Output
        value: $(tasks.knn-train.results.Output)
      - name: odh-pvc-name
        value: $(tasks.odh-pvc.results.name)
      taskSpec:
        steps:
        - name: main
          args:
          - --eval-path
          - $(inputs.params.knn-train-Output)
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def knn_inference (eval_path):\n    import numpy as np\n    import pandas\
            \ as pd\n    import argparse\n    import pickle\n    from sklearn.neighbors\
            \ import KNeighborsRegressor\n    from sklearn.model_selection import\
            \ train_test_split\n    from sklearn.preprocessing import StandardScaler\n\
            \    from sklearn.model_selection import RandomizedSearchCV\n    from\
            \ sklearn.metrics import mean_squared_error\n    print(\"Reading evaluation\
            \ data from\", eval_path)\n    evaluation_data = pd.read_csv(eval_path+'eval.csv')\n\
            \    # Load ss model\n    ss = pickle.load(open(eval_path+'scaler.pkl',\
            \ 'rb'))\n    x_predict = ss.transform(evaluation_data)\n    # Load knn\
            \ regressor\n    knn = pickle.load(open(eval_path+'model.pkl', 'rb'))\n\
            \    # Predict on evaluation data\n    y_predict = knn.predict(x_predict)\n\
            \    # Create dataframe with long, lat, soil moisture\n    out_df = pd.DataFrame(data={'x':evaluation_data['x'].round(decimals=9),\
            \ 'y':evaluation_data['y'].round(decimals=9), 'sm':y_predict})\n    out_df\
            \ = out_df.reindex(['x','y','sm'], axis=1)\n    #Print to file predictions\
            \ \n    out_df.to_csv(eval_path+\"predictions.csv\", index=False, header=False)\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Knn inference',\
            \ description='')\n_parser.add_argument(\"--eval-path\", dest=\"eval_path\"\
            , type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args =\
            \ vars(_parser.parse_args())\n\n_outputs = knn_inference(**_parsed_args)\n"
          image: olayap/somospie
          volumeMounts:
          - mountPath: /cos/
            name: odh-pvc
        params:
        - name: knn-train-Output
        - name: odh-pvc-name
        volumes:
        - name: odh-pvc
          persistentVolumeClaim:
            claimName: $(inputs.params.odh-pvc-name)
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Knn inference",
              "outputs": [], "version": "Knn inference@sha256=6f57348e037ec5a30faaabe9aa8584fd6fdc2e9a9bf58e5f8ea95cba7aa6dbc8"}'
